{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c895d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce9fa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a766b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 750 Ti\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc4bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3515bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a70e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runtime(function):\n",
    "    \"\"\"\n",
    "    A wrapper function that calculates the runtime of the specified function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = function(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Runtime for {function.__name__}: {end_time - start_time} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8ff50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ---------------------------   Data cleaning   ---------------------------\n",
    "def clean_text(text):\n",
    "    # Remove twitter Return handles (RT @xxx:)\n",
    "    text = re.sub(\"RT @[\\w]*:\", \"\", text)\n",
    "\n",
    "    # Remove twitter handles (@xxx)\n",
    "    text = re.sub(\"@[\\w]*\", \"\", text)\n",
    "\n",
    "    # Remove URL links (httpxxx)\n",
    "    url_matcher = \"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "    text = re.sub(url_matcher, \"\", text)\n",
    "    \n",
    "    # Remove any multiple white spaces, tabs or newlines\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    \n",
    "    #remove “”\n",
    "    text = re.sub(\"“|”\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#   ---------------------------   Data filtering   ---------------------------\n",
    "\n",
    "# Method 1 filters the posts based on only 1 rule, which is that the ticker of the company of ...\n",
    "# ... which the sentiment is being calculated is present.\n",
    "def filter_data_1(post, ticker):\n",
    "    # Filter out posts that do not mention the company ticker.\n",
    "    if bool(re.search(fr\"\\${ticker}\", post, re.IGNORECASE)):\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "# Method 2 filters the posts based on the rule that exactly 1 ticker is mentioned and ...\n",
    "# ... that this ticker is the ticker of the company of which the sentiment is being calculated   \n",
    "def filter_data_2(post, ticker):\n",
    "    # Count the number of tickers in the post\n",
    "    matches = re.findall(r\"\\$[a-zA-Z]+\", post)\n",
    "    count = len(matches)\n",
    "    \n",
    "    # Filter out posts with more or less than 1 ticker, and check whether this 1 ticker is the company ticker.\n",
    "    if count == 1 and bool(re.search(fr\"\\${ticker}\", post, re.IGNORECASE)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2fd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "@calculate_runtime\n",
    "def calc_sent_finbert(df):\n",
    "    # Inititalise sentiment pipeline\n",
    "    nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # Create sentence list and run finBERT\n",
    "    sentence_list = df['text'].to_list()\n",
    "    results = nlp(sentence_list)\n",
    "\n",
    "    # Add results to main dataframe\n",
    "    results = pd.DataFrame(results)\n",
    "    df = df.merge(results, how='left', left_index=True, right_index=True)\n",
    "    return df  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f0ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def clean_data(df, ticker):\n",
    "    start_time = time.time()\n",
    "    # Drop all non English Tweets and any unnnamed columns\n",
    "    df = df[df['lang'] == 'en']\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # Create some datetime items\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['date'] = df['created_at'].dt.date\n",
    "    df['hour'] = df['created_at'].dt.hour\n",
    "#     df['test'] = df['text'].apply(filter_data_1, ticker=ticker)\n",
    "    \n",
    "    #   ---------------------------   Data cleaning   ---------------------------\n",
    "    # Clean text\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    print(f\"[{ticker}] Done cleaning after --- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    # Drop duplicate tweets based on cleaned text (sometimes Tweets include the same text but different links for example)\n",
    "    df = df.drop_duplicates(subset=['author_id', 'text'], keep=False)\n",
    "    \n",
    "    # Resetting index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #   ---------------------------   Data filter   ---------------------------\n",
    "    # Filter dataframe using both filter 1 and 2 (read above what they mean)\n",
    "    df['filter_1'] = df['text'].apply(filter_data_1, ticker=ticker)\n",
    "    df['filter_2'] = df['text'].apply(filter_data_2, ticker=ticker)\n",
    "    \n",
    "    print(f\"[{ticker}] Done filtering after --- %s seconds ---\" % (time.time() - start_time))    \n",
    "\n",
    "    # #   ---------------------------   Sentiment   ---------------------------\n",
    "    df = calc_sent_finbert(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd08f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_results(df):\n",
    "    # Drop scores with less than 80% certainty\n",
    "    df = df[df['score'] > 0.8]\n",
    "\n",
    "    # Drop all neutral observations\n",
    "    df = df[df['label'] != \"Neutral\"]\n",
    "    df['pos'] = np.where(df['label'] == \"Positive\", 1, 0)\n",
    "    df['neg'] = np.where(df['label'] == \"Negative\", 1, 0)\n",
    "\n",
    "    # Create results_df with [filter_1]\n",
    "    counted_results = df[df['filter_1']][['date', 'pos', 'neg']].groupby('date', as_index=False).sum().rename(columns={\"pos\": \"[f1BERT]pos\", \"neg\": \"[f1BERT]neg\"})\n",
    "    counted_results['[f1BERT]total'] = counted_results['[f1BERT]pos'] + counted_results['[f1BERT]neg']\n",
    "\n",
    "    # Create results_df with [filter_1]\n",
    "    to_merge_df = df[df['filter_2']][['date', 'pos', 'neg']].groupby('date', as_index=False).sum().rename(columns={\"pos\": \"[f2BERT]pos\", \"neg\": \"[f2BERT]neg\"})\n",
    "    to_merge_df['[f2BERT]total'] = to_merge_df['[f2BERT]pos'] + to_merge_df['[f2BERT]neg']\n",
    "    counted_results = counted_results.merge(to_merge_df, how='left', left_on='date', right_on='date')\n",
    "\n",
    "    return counted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_measures(return_df):\n",
    "    # Get results dataframe\n",
    "    sentiment_measures = count_results(return_df)\n",
    "    \n",
    "    # Method 1 - ratio\n",
    "    sentiment_measures['[f1BERT]method_1'] = sentiment_measures['[f1BERT]pos'] / sentiment_measures['[f1BERT]total']\n",
    "    sentiment_measures['[f2BERT]method_1'] = sentiment_measures['[f2BERT]pos'] / sentiment_measures['[f2BERT]total']\n",
    "\n",
    "    # Method 2 - discontinued as it is the same as method 1\n",
    "    #     sentiment_measures['[f1s2]method_2'] = (sentiment_measures['[f1s2]pos'] - sentiment_measures['[f1s2]neg']) / sentiment_measures['[f1s2]total']\n",
    "    #     sentiment_measures['[f2s2]method_2'] = (sentiment_measures['[f2s2]pos'] - sentiment_measures['[f2s2]neg']) / sentiment_measures['[f2s2]total']\n",
    "\n",
    "    # Method 3\n",
    "    # Method 3 does not exist with finBERT\n",
    "\n",
    "    return sentiment_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de2bfe",
   "metadata": {},
   "source": [
    "## Loop tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f578b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/AAPL.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/AMD.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/AMZN.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/ATVI.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/BA.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/BABA.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/BAC.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/DIS.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/F.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/GE.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/GME.csv]\n",
      "[IQ] Done cleaning after --- 1.4421744346618652 seconds ---\n",
      "[IQ] Done filtering after --- 1.79996919631958 seconds ---\n",
      "Runtime for calc_sent_finbert: 729.3307783603668 seconds\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/LULU.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/MSFT.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/MU.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/NFLX.csv]\n",
      "[NVDA] Done cleaning after --- 7.702570199966431 seconds ---\n",
      "[NVDA] Done filtering after --- 9.316661596298218 seconds ---\n",
      "Runtime for calc_sent_finbert: 4137.2296261787415 seconds\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/SBUX.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/SHOP.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/SNAP.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/SQ.csv]\n",
      "[TLRY] Done cleaning after --- 4.40049147605896 seconds ---\n",
      "[TLRY] Done filtering after --- 5.435632944107056 seconds ---\n",
      "Runtime for calc_sent_finbert: 2679.9692051410675 seconds\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/TSLA.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/V.csv]\n",
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/Twitter/sentiment/finBERT/WMT.csv]\n"
     ]
    }
   ],
   "source": [
    "filedir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\Twitter\\merged\"\n",
    "# ticker_list = [']\n",
    "# done_list = []\n",
    "# almost_done = []\n",
    "ticker_list = ['AAPL', 'AMD', 'ATVI', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    csv_path = os.path.join(filedir, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "    save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\Twitter\\sentiment\\finBERT\"\n",
    "    save_path = os.path.join(save_dir, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "    # Check if file already exists and skip sentiment calculation if file exists\n",
    "    if os.path.isfile(save_path):\n",
    "        print(f\"[skipping] File already exists: [{save_path}]\")\n",
    "        \n",
    "    else:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Filter and clean data. Also perform finBERT sentiment scoring.\n",
    "        return_df = clean_data(df, ticker)\n",
    "\n",
    "        # Calculate sentiment scores for each method\n",
    "        sentiment_measures = calc_sent_measures(return_df)\n",
    "        \n",
    "        # Saving the dataframe\n",
    "        sentiment_measures.to_csv(save_path, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ace753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
