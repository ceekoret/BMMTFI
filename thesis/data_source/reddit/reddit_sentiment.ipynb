{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8e5854",
   "metadata": {},
   "source": [
    "# Reddit sentiment <a class=\"anchor\" id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c26d1",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Reddit filters and sentiment methods](#bullet1)\n",
    "* [General functions and imports](#bullet2)\n",
    "    - [Filter methods](#sub-bullet2.1)\n",
    "* [Reddit method 1](#bullet3)\n",
    "* [Reddit method 2](#bullet4)\n",
    "* [Sentiment calculations](#bullet5)\n",
    "    - [VADER sentiment](#sub-bullet5.1)\n",
    "    - [finBERT sentiment](#sub-bullet5.2)\n",
    "    - [Relative volume](#sub-bullet5.3)\n",
    "* [ToDo](#ToDo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404b26c",
   "metadata": {},
   "source": [
    "## Reddit filters and sentiment methods <a class=\"anchor\" id=\"bullet1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61d0ce",
   "metadata": {},
   "source": [
    "Reddit sentiment is calculated in different ways. I use 2 different 'Reddit-methods' and 2 different [relevancy filters](#sub-bullet2.1) to approach the Reddit data. \n",
    "\n",
    "| Filter | Method 1 | Method 2 |\n",
    "| --- | --- | --- |\n",
    "| Filter 1 | m1f1 | m2f1 |\n",
    "| Filter 2 | m1f2 | m2f2 |\n",
    "\n",
    "\n",
    "On these four combinations, two different sentiment analysis methods will be performed.\n",
    "\n",
    "\n",
    "**Reddit methods**\n",
    "- The first way is similar to the method used for Twitter posts. All Reddit comments will be scanned and filtered for stock mentions. These stock mentions are then counted for each stock and subsequent sentiment analysis will be performed on them.\n",
    "- The second method will not only look at comments, but also consider the Reddit posts. This is done in line with [Hu, Jones, Zhang and Zhang (2021) page 10-12](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3807655). The method begins start by checking whether a post or comment contains a company ticker or name. Once such a mention is found, any direct reply's are searched. All posts or comments mentioning a company together with any direct reply's to these posts or comments are counted as social media activity for this particular company. \n",
    "\n",
    "These methods differ slightly from the Reddit comments. The reason for this is that it seems that the `$ticker` naming convention does not seem popular on Reddit. Searching only for `$tsla` returns only 58 observations for the r/stocks subreddit during the 2020_07 period. When searching for the company name instead (Tesla), 4192 observations are found. Hence Reddit comments will also be searched using company names.\n",
    "\n",
    "\n",
    "A total of `20.511.418` comments are scraped from the 5 subreddits investing, pennystocks, stockmarket, stocks and wallstreetbets. `844.715` comments posts are found for method 1. Method 2 ... `36.244` posts\n",
    "\n",
    "**Sentiment methods**\n",
    "The sentiment methods which are used are:\n",
    "- cjhutto VADER sentiment\n",
    "- finBERT sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468761a2",
   "metadata": {},
   "source": [
    "## General functions and imports <a class=\"anchor\" id=\"bullet2\"></a>\n",
    "\n",
    "[Go back up](#top)\n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf75eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import statsmodels.formula.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb07310",
   "metadata": {},
   "source": [
    "**General functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af720a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GameStop'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of company names\n",
    "company_names = {'TSLA': 'Tesla', \n",
    "                 'MU': 'Micron Technology', \n",
    "                 'SNAP': 'Snapchat', \n",
    "                 'AMD': 'AMD', \n",
    "                 'DIS': 'Disney', \n",
    "                 'MSFT': 'Microsoft', \n",
    "                 'AAPL': 'Apple', \n",
    "                 'AMZN': 'Amazon', \n",
    "                 'SQ': 'Block', \n",
    "                 'BABA': 'Alibaba', \n",
    "                 'V': 'Visa', \n",
    "                 'NFLX': 'Netflix', \n",
    "                 'IQ': 'IQIYI', \n",
    "                 'ATVI': 'Activision Blizzard', \n",
    "                 'SHOP': 'Shopify', \n",
    "                 'BA': 'Boeing', \n",
    "                 'NVDA': 'NVIDIA', \n",
    "                 'GE': 'General Electric', \n",
    "                 'WMT': 'Walmart', \n",
    "                 'SBUX': 'Starbucks', \n",
    "                 'F': 'Ford', \n",
    "                 'TLRY': 'Tilray', \n",
    "                 'LULU': 'Lululemon', \n",
    "                 'BAC': 'Bank of America', \n",
    "                 'GME': 'GameStop'}\n",
    "\n",
    "# Return name of company name dict\n",
    "def find_company_name(ticker):\n",
    "    ticker = ticker.upper()\n",
    "    \n",
    "    return company_names[ticker]\n",
    "    \n",
    "    \n",
    "find_company_name(\"gme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5463b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of top NASDAQ and S&P companies\n",
    "company_list = ['MetLife', 'Exelon', \"O'Reilly Automotive\", 'Baker Hughes', 'Rivian',\n",
    "                'Applied Materials', 'Palo Alto Networks', 'Alphabet', 'Nike',\n",
    "                'ASML', 'Lululemon', 'Bank of America', 'Cadence Design Systems',\n",
    "                'BNY Mellon', 'Pfizer', 'Cintas', 'Google', 'Marriott International', 'American Tower',\n",
    "                'Thermo Fisher Scientific', 'CrowdStrike', 'Ross Stores', 'Emerson', 'Lilly', 'Linde',\n",
    "                'Salesforce', 'Qualcomm', 'CoStar Group', 'T-Mobile','TMobile', 'Caterpillar', 'Cognizant',\n",
    "                'FedEx', 'JD.com', 'Johnson & Johnson', 'Alibaba', 'Netflix', 'Seagen', 'Sirius XM',\n",
    "                'Procter & Gamble', 'Microchip Technology', 'PepsiCo', 'Nvidia', 'Dow', 'Zscaler',\n",
    "                'Abbott', 'Charter Communications', 'Charles Schwab', \"McDonald's\", 'Mastercard',\n",
    "                'Ford', 'MercadoLibre', 'Booking Holdings', 'Diamondback Energy', 'Dollar Tree',\n",
    "                'Verisk', 'Zoom Video Communications', 'Altria', 'IQIYI', 'Constellation Energy',\n",
    "                'Wells Fargo', 'Starbucks', 'NXP', 'Adobe', 'eBay', 'Gilead', 'American Electric Power',\n",
    "                'Lucid Motors', 'Disney', 'Coca-Cola', 'Visa', 'Illumina, Inc.', 'Moderna', 'Fiserv',\n",
    "                'Boeing', 'Medtronic', 'Gilead Sciences', 'GlobalFoundries', 'NextEra Energy',\n",
    "                'Texas Instruments', 'Intel', 'Monster Beverage', 'Block', 'Meta Platforms', 'Tesla',\n",
    "                'KLA Corporation', 'Broadcom', 'Airbnb', 'PayPal', 'CVS Health', 'AT&T', 'Cisco',\n",
    "                'Raytheon Technologies', 'U.S. Bank', 'Duke Energy', 'Union Pacific', '3M',\n",
    "                'Lockheed Martin', 'Ansys', 'ExxonMobil', 'Verizon', 'Xcel Energy', 'Paccar', 'Costco',\n",
    "                'Analog Devices', 'AbbVie', 'Lam Research', 'Vertex Pharmaceuticals', 'Intuitive Surgical',\n",
    "                'Synopsys', 'AMD', 'American Express', 'Regeneron', 'Activision Blizzard', 'Target',\n",
    "                'Atlassian', 'Advanced Micro Devices', 'Warner Bros. Discovery', 'Tilray', 'BlackRock',\n",
    "                'Amazon', 'Walmart', 'Workday', 'Dr Pepper', 'JPMorgan Chase', 'Meta', 'Copart',\n",
    "                'Biogen', 'Paychex', 'Capital One', 'UnitedHealth Group', 'Fastenal', 'Microsoft',\n",
    "                'AstraZeneca', \"Lowe's\", 'Datadog', 'General Electric', 'ADP', 'Goldman Sachs', 'PDD Holdings',\n",
    "                'Accenture', 'Mondelēz International', 'United Parcel Service', 'Comcast', 'Micron Technology',\n",
    "                'CSX Corporation', 'Electronic Arts', 'Marvell Technology', 'ConocoPhillips',\n",
    "                'Walgreens Boots Alliance', 'Home Depot', 'Idexx Laboratories', 'IBM', 'Oracle', 'Shopify',\n",
    "                'Apple', 'GE', 'Morgan Stanley', 'Merck', 'Citigroup', 'Simon', 'Amgen',\n",
    "                'Philip Morris International', 'General Dynamics', 'Bristol Myers Squibb', \n",
    "                'Old Dominion Freight Line', 'Snapchat', 'GameStop', 'Align Technology', 'Colgate-Palmolive', \n",
    "                'Berkshire Hathaway', 'Enphase Energy', 'Fortinet', 'Intuit', 'Danaher', 'Kraft Heinz', 'Chevron', \n",
    "                'GM', 'DexCom', 'Southern Company', 'Autodesk', 'American International Group', 'Honeywell']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63867ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"\\bMetLife\\b|\\bExelon\\b|\\bO'Reilly Automotive\\b|\\bBaker Hughes\\b|\\bRivian\\b|\\bApplied Materials\\b|\\bPalo Alto Networks\\b|\\bAlphabet\\b|\\bNike\\b|\\bASML\\b|\\bLululemon\\b|\\bBank of America\\b|\\bCadence Design Systems\\b|\\bBNY Mellon\\b|\\bPfizer\\b|\\bCintas\\b|\\bGoogle\\b|\\bMarriott International\\b|\\bAmerican Tower\\b|\\bThermo Fisher Scientific\\b|\\bCrowdStrike\\b|\\bRoss Stores\\b|\\bEmerson\\b|\\bLilly\\b|\\bLinde\\b|\\bSalesforce\\b|\\bQualcomm\\b|\\bCoStar Group\\b|\\bT-Mobile\\b|\\bTMobile\\b|\\bCaterpillar\\b|\\bCognizant\\b|\\bFedEx\\b|\\bJD.com\\b|\\bJohnson & Johnson\\b|\\bAlibaba\\b|\\bNetflix\\b|\\bSeagen\\b|\\bSirius XM\\b|\\bProcter & Gamble\\b|\\bMicrochip Technology\\b|\\bPepsiCo\\b|\\bNvidia\\b|\\bDow\\b|\\bZscaler\\b|\\bAbbott\\b|\\bCharter Communications\\b|\\bCharles Schwab\\b|\\bMcDonald's\\b|\\bMastercard\\b|\\bFord\\b|\\bMercadoLibre\\b|\\bBooking Holdings\\b|\\bDiamondback Energy\\b|\\bDollar Tree\\b|\\bVerisk\\b|\\bZoom Video Communications\\b|\\bAltria\\b|\\bIQIYI\\b|\\bConstellation Energy\\b|\\bWells Fargo\\b|\\bStarbucks\\b|\\bNXP\\b|\\bAdobe\\b|\\beBay\\b|\\bGilead\\b|\\bAmerican Electric Power\\b|\\bLucid Motors\\b|\\bDisney\\b|\\bCoca-Cola\\b|\\bVisa\\b|\\bIllumina, Inc.\\b|\\bModerna\\b|\\bFiserv\\b|\\bBoeing\\b|\\bMedtronic\\b|\\bGilead Sciences\\b|\\bGlobalFoundries\\b|\\bNextEra Energy\\b|\\bTexas Instruments\\b|\\bIntel\\b|\\bMonster Beverage\\b|\\bBlock\\b|\\bMeta Platforms\\b|\\bTesla\\b|\\bKLA Corporation\\b|\\bBroadcom\\b|\\bAirbnb\\b|\\bPayPal\\b|\\bCVS Health\\b|\\bAT&T\\b|\\bCisco\\b|\\bRaytheon Technologies\\b|\\bU.S. Bank\\b|\\bDuke Energy\\b|\\bUnion Pacific\\b|\\b3M\\b|\\bLockheed Martin\\b|\\bAnsys\\b|\\bExxonMobil\\b|\\bVerizon\\b|\\bXcel Energy\\b|\\bPaccar\\b|\\bCostco\\b|\\bAnalog Devices\\b|\\bAbbVie\\b|\\bLam Research\\b|\\bVertex Pharmaceuticals\\b|\\bIntuitive Surgical\\b|\\bSynopsys\\b|\\bAMD\\b|\\bAmerican Express\\b|\\bRegeneron\\b|\\bActivision Blizzard\\b|\\bTarget\\b|\\bAtlassian\\b|\\bAdvanced Micro Devices\\b|\\bWarner Bros. Discovery\\b|\\bTilray\\b|\\bBlackRock\\b|\\bAmazon\\b|\\bWalmart\\b|\\bWorkday\\b|\\bDr Pepper\\b|\\bJPMorgan Chase\\b|\\bMeta\\b|\\bCopart\\b|\\bBiogen\\b|\\bPaychex\\b|\\bCapital One\\b|\\bUnitedHealth Group\\b|\\bFastenal\\b|\\bMicrosoft\\b|\\bAstraZeneca\\b|\\bLowe's\\b|\\bDatadog\\b|\\bGeneral Electric\\b|\\bADP\\b|\\bGoldman Sachs\\b|\\bPDD Holdings\\b|\\bAccenture\\b|\\bMondelēz International\\b|\\bUnited Parcel Service\\b|\\bComcast\\b|\\bMicron Technology\\b|\\bCSX Corporation\\b|\\bElectronic Arts\\b|\\bMarvell Technology\\b|\\bConocoPhillips\\b|\\bWalgreens Boots Alliance\\b|\\bHome Depot\\b|\\bIdexx Laboratories\\b|\\bIBM\\b|\\bOracle\\b|\\bShopify\\b|\\bApple\\b|\\bGE\\b|\\bMorgan Stanley\\b|\\bMerck\\b|\\bCitigroup\\b|\\bSimon\\b|\\bAmgen\\b|\\bPhilip Morris International\\b|\\bGeneral Dynamics\\b|\\bBristol Myers Squibb\\b|\\bOld Dominion Freight Line\\b|\\bSnapchat\\b|\\bGameStop\\b|\\bAlign Technology\\b|\\bColgate-Palmolive\\b|\\bBerkshire Hathaway\\b|\\bEnphase Energy\\b|\\bFortinet\\b|\\bIntuit\\b|\\bDanaher\\b|\\bKraft Heinz\\b|\\bChevron\\b|\\bGM\\b|\\bDexCom\\b|\\bSouthern Company\\b|\\bAutodesk\\b|\\bAmerican International Group\\b|\\bHoneywell\\b\",\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('|'.join(['|'.join([f\"\\\\b{name}\\\\b\" for name in company_list])]))\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d81e38",
   "metadata": {},
   "source": [
    "### Filter methods <a class=\"anchor\" id=\"sub-bullet2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe8c32",
   "metadata": {},
   "source": [
    "Besides the two [Reddit-methods](#bullet1) which were described in the introduction, I also make use of two different filter. These filters are used to check if a Tweet is relevant to a specific company.\n",
    "\n",
    "To summarize:\n",
    "- `filter_data_1` Checks whether the name of the company or the ticker of the company is mentioned. If one of these is true, the comment or post is seen as relevant.\n",
    "- `filter_data_2` Is more strict that filter_data_1. Just like the filter 1, is checks for the company name and ticker. However, this time it also checks whether other tickers or company names are mentioned. It does this using NASDAQ and S&P company names which are summarized in `company_list`. These company names are then compiled into a regex search. If it turns out that different tickers or company names are mentioned in the post or comment, the post or comment is not considered relevant. If only the company ticker or company's name is mentioned, the post <u>is</u> considered relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e9c2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Functions borrowed from Twitter 'Data cleaning and sentiment' Jupyter Notebook\n",
    "def clean_text(text):\n",
    "    # Remove twitter Return handles (RT @xxx:)\n",
    "    text = re.sub(\"RT @[\\w]*:\", \"\", text)\n",
    "\n",
    "    # Remove twitter handles (@xxx)\n",
    "    text = re.sub(\"@[\\w]*\", \"\", text)\n",
    "\n",
    "    # Remove URL links (httpxxx)\n",
    "    url_matcher = \"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "    text = re.sub(url_matcher, \"\", text)\n",
    "    \n",
    "    # Remove any multiple white spaces, tabs or newlines\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    \n",
    "    #remove “”\n",
    "    text = re.sub(\"“|”\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# This function is slightly adjusted. Not only are company tickers searched (\"$ticker\"), but also company names are searched.\n",
    "# This change increases the amount of mentions for Tesla on the r/stocks subreddit during the 2020_07 period from 58 to 4192 observations\n",
    "def filter_data_1(post, ticker):\n",
    "    # Filter out posts that do not mention the company ticker.\n",
    "    company_name = find_company_name(ticker)\n",
    "    if bool(re.search(fr\"(\\${ticker})|({company_name})\", post, re.IGNORECASE)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Method 2 filters the comments based on the rule that exactly 1 ticker or company name is mentioned and ...\n",
    "# ... that this ticker is the ticker of the company of which the sentiment is being calculated   \n",
    "def filter_data_2(post, ticker):\n",
    "    # ---- Tickers ----\n",
    "    # Count the number of tickers in the post\n",
    "    ticker_matches = len(re.findall(r\"\\$[a-zA-Z]+\", post, re.IGNORECASE))\n",
    "    company_ticker_matches = len(re.findall(fr\"\\${ticker}\", post, re.IGNORECASE))\n",
    "    \n",
    "    # The ticker_diff needs to be equal to zero, else other tickers than the company ticker are mentioned.\n",
    "    ticker_diff = ticker_matches - company_ticker_matches\n",
    "    \n",
    "    # print(f\"{len(ticker_matches)} - {len(company_ticker_matches)} = {ticker_diff}\")\n",
    "    \n",
    "    # ---- Company name ----\n",
    "    # Create pattern which matches any of the company names in the company_list\n",
    "    # '\\\\b' prevents any occurences of short company names from being picked up mid-string:\n",
    "    # GE --> 'Vegetable' would be picked up without this rule\n",
    "    pattern = re.compile('|'.join(['|'.join([f\"\\\\b{name}\\\\b\" for name in company_list])]))\n",
    "    name_matches = len(pattern.findall(post, re.IGNORECASE))\n",
    "    \n",
    "    # Next the company of the ticker is searched.\n",
    "    company_name = find_company_name(ticker)\n",
    "    company_name_matches = len(re.findall(f\"\\\\b{company_name}\\\\b\", post, re.IGNORECASE))\n",
    "    \n",
    "    # The company_name_diff needs to be equal to zero, else other tickers than the company ticker are mentioned.\n",
    "    company_name_diff = name_matches - company_name_matches\n",
    "    \n",
    "    # print(f\"{name_matches_count} - {len(company_name_matches)} = {company_name_diff}\")\n",
    "    \n",
    "    # Filter out posts with more or less than 1 ticker, and check whether this 1 ticker is the company ticker.\n",
    "    if ticker_diff == 0 and company_name_diff == 0 and (company_ticker_matches > 0 or company_name_matches > 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "text = \"This is a test text. If Apple is in the text, without any other companies mentioned, the functions returns True.\"\n",
    "print(filter_data_2(text, \"aapl\"))\n",
    "\n",
    "text = \"If another company is mentioned, like Gamestop, it returns False\"\n",
    "print(filter_data_2(text, \"aapl\"))\n",
    "\n",
    "text = \"If no company is mentioned, it also returns False\"\n",
    "print(filter_data_2(text, \"aapl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd8c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c88a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mentions(df, ticker, social_type='comment', filter_type=1):\n",
    "    if social_type == 'comment':\n",
    "        # Dropping any potential NA's\n",
    "        df = df[df['body'].notna()]\n",
    "\n",
    "        if filter_type == 1:\n",
    "            # Filter out comments that do not mention company ticker using 'filter_data_1'\n",
    "            df_filter = df['body'].apply(filter_data_1, ticker=ticker)\n",
    "        elif filter_type == 2:\n",
    "            # Filter out comments that do not mention company ticker using 'filter_data_2'\n",
    "            df_filter = df['body'].apply(filter_data_2, ticker=ticker)\n",
    "        else:\n",
    "            raise Exception(\"Please select a valid filter_type for func [filter_mentions]. Either 1 or 2.\")\n",
    "        df = df[df_filter]\n",
    "        \n",
    "        # Skip cleaning if df is empty\n",
    "        if df.shape[0] > 0:\n",
    "            # Clean text\n",
    "            df['body'] = df['body'].apply(clean_text)  \n",
    "            \n",
    "    elif social_type == 'post':\n",
    "        # Dropping any potential NA's\n",
    "        df = df[df['selftext'].notna()]\n",
    "\n",
    "        if filter_type == 1:\n",
    "            # Filter out posts that do not mention company ticker using 'filter_data_1'\n",
    "            selftext_filter = df['selftext'].apply(filter_data_1, ticker=ticker)\n",
    "            title_filter = df['title'].apply(filter_data_1, ticker=ticker)\n",
    "        elif filter_type == 2:\n",
    "            # Filter out posts that do not mention company ticker using 'filter_data_2'\n",
    "            selftext_filter = df['selftext'].apply(filter_data_2, ticker=ticker)\n",
    "            title_filter = df['title'].apply(filter_data_2, ticker=ticker)\n",
    "        else:\n",
    "            raise Exception(\"Please select a valid filter_type for func [filter_mentions]. Either 1 or 2.\")\n",
    "        df = df[selftext_filter | title_filter]\n",
    "        \n",
    "        \n",
    "        # Skip cleaning if df is empty\n",
    "        if df.shape[0] > 0:\n",
    "            # Clean text\n",
    "            df['selftext'] = df['selftext'].apply(clean_text)\n",
    "           \n",
    "        # For posts specifically, I drop observations for\n",
    "        # - Authors postings multiple posts on 1 day (spam or duped posts)\n",
    "        df = df.drop_duplicates(subset=['author', 'date'], keep=False)\n",
    "        # - Posts with duplicate texts\n",
    "        df = df.drop_duplicates(subset=['selftext'], keep=False)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Please select a valid social_type for func [filter_mentions]. Either 'comment' or 'post'.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_df(df, save_path):\n",
    "    # Check if file already exists. Ignore headers if true\n",
    "    if os.path.isfile(save_path):\n",
    "        df.to_csv(save_path, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(save_path, encoding='utf-8', index=False)\n",
    "        print(f\"Creating new file at [{save_path}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d4df5",
   "metadata": {},
   "source": [
    "## Reddit method 1 <a class=\"anchor\" id=\"bullet3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9fba0",
   "metadata": {},
   "source": [
    "**Extract all comments which mention a company ticker or name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeae60fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AAPL.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AMD.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AMZN.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/ATVI.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BA.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BABA.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BAC.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/DIS.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/F.csv]nfiltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/GE.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/GME.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/IQ.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/LULU.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/MSFT.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/MU.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/NFLX.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/NVDA.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SBUX.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SHOP.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SNAP.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SQ.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/TLRY.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/TSLA.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/V.csv]nfiltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/WMT.csv]iltered\\investing\\2018_04.csv]\n",
      "Time passed: 35953.8  - Processing WMT  for [E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\\wallstreetbets\\2020_08.csv]\r"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\"\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "\n",
    "if save:\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Dropping certain columns\n",
    "            df.drop(columns=['score_hidden', 'total_awards_received', 'nest_level', 'author_fullname'], inplace=True)\n",
    "            \n",
    "            # Loop tickers\n",
    "            for ticker in ticker_list:\n",
    "                print(f\"Time passed: {str(round((time.time() - start_time), 1)).ljust(8)} - Processing {ticker.ljust(4)} for [{csv_path}]\", end='\\r')\n",
    "\n",
    "                # ----------------------- Filter_1 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(work_df, ticker, social_type='comment', filter_type=1)\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)\n",
    "\n",
    "                # ----------------------- Filter_2 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(work_df, ticker, social_type='comment', filter_type=2)\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c773b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae6caab8",
   "metadata": {},
   "source": [
    "## Reddit method 2 <a class=\"anchor\" id=\"bullet4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26e456",
   "metadata": {},
   "source": [
    "Method 2 checks what posts contain mentions of companies in the form of ticker or company name mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e90f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/AAPL.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/AAPL.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/AMD.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/AMD.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/AMZN.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/AMZN.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/ATVI.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/ATVI.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/BA.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/BA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/BABA.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/BABA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/BAC.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/BAC.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/DIS.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/DIS.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/F.csv]nfiltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/F.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/GE.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/GE.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/GME.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/GME.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/IQ.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/IQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/LULU.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/LULU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/MSFT.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/MSFT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/MU.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/MU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/NFLX.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/NFLX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/NVDA.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/NVDA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SBUX.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SBUX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SHOP.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SHOP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SNAP.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SNAP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SQ.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/TLRY.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/TLRY.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/TSLA.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/TSLA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/V.csv]nfiltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/V.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/WMT.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/WMT.csv]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\unfiltered\"\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\filtered\"\n",
    "\n",
    "if save:\n",
    "    start_time = time.time()\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Loop tickers\n",
    "            for ticker in ticker_list:\n",
    "                print(f\"Time passed: {str(round((time.time() - start_time), 1)).ljust(8)} - Processing {ticker.ljust(4)} for [{csv_path}]\", end='\\r')\n",
    "                # ----------------------- Filter_1 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(df, ticker, social_type='post', filter_type=1)\n",
    "\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)\n",
    "\n",
    "                # ----------------------- Filter_2 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(work_df, ticker, social_type='post', filter_type=2)\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c7c88",
   "metadata": {},
   "source": [
    "**Merging all comments to a single file**\n",
    "\n",
    "After finding all posts mentioning a company (besides the comments as done under method 1), it is now time to see what replies are posted to these comments. For posts these replies will be top level comments. For replies on other comments, they can be found on any level. \n",
    "\n",
    "I begin by merging all comments into a single file. This does not serve any purpose besides making it easier to loop the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93649717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "# --- Merge all comments into single file ---\n",
    "if save:\n",
    "    rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\"\n",
    "    save_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv\"\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            df.drop(columns=['score_hidden', 'total_awards_received', 'nest_level', 'author_fullname'], inplace=True)\n",
    "\n",
    "            # Save new csv or append to existing csv\n",
    "            save_df(df, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3fb30",
   "metadata": {},
   "source": [
    "**Searching for replies**\n",
    "\n",
    "Method 2 also finds all comments which are replies to posts or comments containing the company name or ticker. \n",
    "\n",
    "To find out whether a comment is a reply, each comment will be checked to see if the `parent_id` matches an `id` of a comment containing a company mention. For responses to posts, the `parent_id` will be empty. For these, the `link_id` will need to match the post's id.\n",
    "\n",
    "For replies to comments:\n",
    "- `df['parent_id']` matches an `df['id']` of a comment containing a company mention.\n",
    "\n",
    "For top-level replies to posts:\n",
    "- `df['parent_id']` is empty and `df['link_id']` is equal to the post's `df['id']` which mentions a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbca6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_replies(df_chunk, comments_df, posts_df):\n",
    "    #           --- Comment filter ---\n",
    "    # Create a dataframe filter which checks if comment is a reply to other comment\n",
    "    comment_id_list = comments_df['id'].to_list()\n",
    "    comment_filter = df_chunk['parent_id'].isin(comment_id_list)\n",
    "    \n",
    "    #           --- Post filter ---\n",
    "    # Removing 't3_' from the comment 'link_id' field\n",
    "    df_chunk['link_id'] = df_chunk['link_id'].str[3:]\n",
    "    \n",
    "    # Create a dataframe filter which checks if comment is a reply to other comment\n",
    "    post_id_list = posts_df['id'].to_list()\n",
    "    post_filter = (df_chunk['parent_id'].isna() & df_chunk['link_id'].isin(post_id_list))\n",
    "    \n",
    "    return_df = df_chunk[comment_filter | post_filter]\n",
    "    return return_df\n",
    "\n",
    "# Checks replies for the chunk dataframe input\n",
    "def check_replies(df):\n",
    "    \n",
    "    ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "    comment_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "    posts_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\filtered\"\n",
    "    save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\replies\"\n",
    "    \n",
    "    for ticker in ticker_list:\n",
    "        # ----------------------- Filter_1 -----------------------\n",
    "        work_df = df.copy()\n",
    "        # Creating paths\n",
    "        comment_path = os.path.join(comment_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        posts_path = os.path.join(posts_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        \n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        posts_df = pd.read_csv(posts_path)\n",
    "        \n",
    "        # Run function\n",
    "        return_df = filter_replies(work_df, comments_df, posts_df)\n",
    "\n",
    "        # Creating save_path and saving file (either new file or appending)\n",
    "        save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_df(return_df, save_path)\n",
    "\n",
    "        # ----------------------- Filter_2 -----------------------\n",
    "        work_df = df.copy()\n",
    "        # Creating paths\n",
    "        comment_path = os.path.join(comment_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        posts_path = os.path.join(posts_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        posts_df = pd.read_csv(posts_path)\n",
    "        \n",
    "        # Run function\n",
    "        return_df = filter_replies(work_df, comments_df, posts_df)\n",
    "\n",
    "        # Creating save_path and saving file (either new file or appending)\n",
    "        save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_df(return_df, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1734d07",
   "metadata": {},
   "source": [
    "**Search for replies by chunking all_comments.csv**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8f6c5",
   "metadata": {},
   "source": [
    "To loop all comments, I loop the chunks from the all_comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d7e36f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/AAPL.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/AAPL.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/AMD.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/AMD.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/AMZN.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/AMZN.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/ATVI.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/ATVI.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/BA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/BA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/BABA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/BABA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/BAC.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/BAC.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/DIS.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/DIS.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/F.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/F.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/GE.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/GE.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/GME.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/GME.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/IQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/IQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/LULU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/LULU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/MSFT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/MSFT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/MU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/MU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/NFLX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/NFLX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/NVDA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/NVDA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SBUX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SBUX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SHOP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SHOP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SNAP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SNAP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/TLRY.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/TLRY.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/TSLA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/TSLA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/V.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/V.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/WMT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/WMT.csv]\n",
      "Processing chunk 21 --- time passed: 2144.2\r"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv\"\n",
    "chunksize = 10 ** 6\n",
    "counter = 1\n",
    "\n",
    "if save:\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize):\n",
    "        print(f\"Processing chunk {str(counter).ljust(2)} --- time passed: {round((time.time() - start_time), 1)}\", end='\\r')\n",
    "        check_replies(chunk)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c871618",
   "metadata": {},
   "source": [
    "**Merging comments with replies**\n",
    "\n",
    "Besides this, duplicates are also removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e55e7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "comment_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "reply_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\replies\"\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\method_2\"\n",
    "\n",
    "if save:\n",
    "    for ticker in ticker_list:\n",
    "        # ----------------------- Filter_1 -----------------------\n",
    "        comment_path = os.path.join(comment_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        reply_path = os.path.join(reply_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        reply_df = pd.read_csv(reply_path)\n",
    "\n",
    "        # Merge files\n",
    "        merged_df = pd.concat([comments_df, reply_df], ignore_index=True)\n",
    "\n",
    "        # Delete duplicates\n",
    "        merged_df.drop_duplicates(subset=['permalink'], inplace=True)\n",
    "\n",
    "        # Saving file\n",
    "        merged_df.to_csv(save_path, encoding='utf-8', index=False)\n",
    "\n",
    "        # ----------------------- Filter_2 -----------------------\n",
    "        comment_path = os.path.join(comment_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        reply_path = os.path.join(reply_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        reply_df = pd.read_csv(reply_path)\n",
    "\n",
    "        # Merge files\n",
    "        merged_df = pd.concat([comments_df, reply_df], ignore_index=True)\n",
    "\n",
    "        # Delete duplicates\n",
    "        merged_df.drop_duplicates(subset=['permalink'], inplace=True) \n",
    "\n",
    "        # Saving file\n",
    "        merged_df.to_csv(save_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11154526",
   "metadata": {},
   "source": [
    "## Sentiment calculations <a class=\"anchor\" id=\"bullet5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d28f7",
   "metadata": {},
   "source": [
    "Having the results for all four different 'Reddit-method' - filter combinations, I can now start with sentiment calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2ed85",
   "metadata": {},
   "source": [
    "### VADER sentiment <a class=\"anchor\" id=\"sub-bullet5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb251d9",
   "metadata": {},
   "source": [
    "**Functions**\n",
    "\n",
    "These functions are partially copied from the Twitter sentiment Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84282be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding word-sentiment pairs to the cjhutto vaderSentiment library.\n",
    "new_words = {}\n",
    "\n",
    "# Adding custom postive words\n",
    "positive_words = {\n",
    "    'buy': 2.0,\n",
    "    'buying': 2.0,\n",
    "    'bullish': 2.0,\n",
    "    'long': 1.0,\n",
    "    'call': 1.0,\n",
    "    'calls': 1.0,\n",
    "    'rocket': 3.0,        # Added for 'rocket' emoji 🚀\n",
    "    'increasing': 2.0,     # Added for 'chart increasing' emoji 📈\n",
    "    'to the moon': 2.5,\n",
    "    \"undervalued\": 2.0\n",
    "}\n",
    "# Adding custom negative words\n",
    "negative_words = {\n",
    "    'decreasing': -2.0,   # Added for 'chart increasing' emoji 📉\n",
    "    'sell': -2.0,\n",
    "    'selling': -2.0,\n",
    "    'bearish': -2.0,\n",
    "    'put': -1,\n",
    "    'puts': -1,\n",
    "    'short': -1.0,\n",
    "    'shorting': -1.5,\n",
    "    \"overvalued\": -2.0,\n",
    "    'expensive': -1.5\n",
    "}\n",
    "\n",
    "# Adding positive and negative words to new_worddictionary\n",
    "new_words.update(positive_words)\n",
    "new_words.update(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acbe51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ---------------------------   Sentiment   ---------------------------\n",
    "# Creating SIA, which uses standard words.\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calc_sentiment_1(text, sent_type):\n",
    "    result = SIA.polarity_scores(text)\n",
    "    return result[sent_type]\n",
    "\n",
    "# Creating SIA2 to add custom words.\n",
    "SIA2 = SentimentIntensityAnalyzer()\n",
    "SIA2.lexicon.update(new_words)\n",
    "\n",
    "def calc_sentiment_2(text, sent_type):\n",
    "    result = SIA2.polarity_scores(text)\n",
    "    return result[sent_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d93d7",
   "metadata": {},
   "source": [
    "**finBERT filter and table deletion** <a class=\"anchor\" id=\"delete-table-info2\"></a>\n",
    "\n",
    "During the preperation of the next sentiment method, I discover that the finBERT has a maximum limit of 512 words. Unlike the Twitter posts, Reddit comments can actually exceed this amount. Hence I decide to filter the 5% largest post. Besides this, to put as many useful words in the 512 word limit, I delelete tables from user's comments. More info about this can be found [here](#delete-table-info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2c86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes any tables from reddit comments.\n",
    "def clean_text(text): \n",
    "    # Remove tables\n",
    "    text = re.sub(\"[^\\|\\s]{1,20}\\s*(?=\\|)|(?<=\\|)\\s*[^\\|]{1,30}\\s*(?=\\|)|(?<=\\|)\\s*[^\\|\\s]{1,30}\", \"\", text)\n",
    "    text = re.sub(\"\\|\", \"\", text)\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92d70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finBERT_filter(df):\n",
    "    # Dropping any NA in body\n",
    "    df.dropna(subset=['body'], inplace=True)\n",
    "        \n",
    "    # Cleaning tables from posts\n",
    "    df['body'] = df['body'].apply(clean_text)\n",
    "\n",
    "    # Getting text lengths  \n",
    "    df['len'] = df['body'].str.len()\n",
    "\n",
    "    # Dropping texts above the 95% quantile\n",
    "    quantile_95 = df['len'].quantile(0.95)\n",
    "    df = df[df['len'] < quantile_95]\n",
    "    \n",
    "    # Resetting index\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a0fcf",
   "metadata": {},
   "source": [
    "**Main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9699b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, ticker):\n",
    "    start_time = time.time()\n",
    "    # Prepping data by setting date column\n",
    "    df['created_at'] = pd.to_datetime(df['utc_datetime_str'])\n",
    "    df['date'] = df['created_at'].dt.date\n",
    "    \n",
    "    # Perform the finBERT filter (see description above)\n",
    "    df = finBERT_filter(df).copy()\n",
    "    \n",
    "    #   ---------------------------   Sentiment   ---------------------------\n",
    "    # Calculate sentiment scores\n",
    "    df[f'compound_sent_1'] = df['body'].astype(str).apply(calc_sentiment_1, sent_type='compound')\n",
    "    df[f'compound_sent_2'] = df['body'].astype(str).apply(calc_sentiment_2, sent_type='compound')\n",
    "    \n",
    "    print(f\"[{ticker}] Done calculating sentiment after --- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    \"\"\"Converting this to pos, neg or neu sentiment\n",
    "    - positive sentiment: compound score >= 0.05\n",
    "    - neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    - negative sentiment: compound score <= -0.05\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if sentiment corresponds to pos, neg or neu sentiment for compound_sent_1\n",
    "    df['s1_pos'] = np.where(df['compound_sent_1'] >= 0.05, 1, 0)\n",
    "    df['s1_neg'] = np.where(df['compound_sent_1'] <= -0.05, 1, 0)\n",
    "\n",
    "    # Check if sentiment corresponds to pos, neg or neu sentiment for compound_sent_2\n",
    "    df['s2_pos'] = np.where(df['compound_sent_2'] >= 0.05, 1, 0)\n",
    "    df['s2_neg'] = np.where(df['compound_sent_2'] <= -0.05, 1, 0)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47f34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_posts(df):\n",
    "    # Create results_df with [sentiment_1]\n",
    "    results_df = df[['date', 's1_pos', 's1_neg']].groupby('date', as_index=False).sum().rename(columns={\"s1_pos\": \"[s1]pos\", \"s1_neg\": \"[s1]neg\"})\n",
    "    results_df['[s1]total'] = results_df['[s1]pos'] + results_df['[s1]neg']\n",
    "\n",
    "    # Merge [sentiment_2]\n",
    "    to_merge_df = df[['date', 's2_pos', 's2_neg']].groupby('date', as_index=False).sum().rename(columns={\"s2_pos\": \"[s2]pos\", \"s2_neg\": \"[s2]neg\"})\n",
    "    to_merge_df['[s2]total'] = to_merge_df['[s2]pos'] + to_merge_df['[s2]neg']\n",
    "    results_df = results_df.merge(to_merge_df, how='left', left_on='date', right_on='date')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eedc96",
   "metadata": {},
   "source": [
    "**Different types of VADER sentiment measures** <a class=\"anchor\" id=\"sentiment-measure\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce24d7",
   "metadata": {},
   "source": [
    "Although different data filters and sentiment measuring methods are mentioned, this does not solve the problem of how sentiment needs to be measured. Do I take the total amount of positive posts as a proxy of sentiment? Or do I use the ratio of positive and negative posts. Or do I subtract the negative posts from the positive posts and divide that figure by the total amount of posts?\n",
    "\n",
    "It seems that there is no clear winner here. I propose that sentiment should be measure in two parts. The first part of the measure should contain the actual `sentiment`. Is it positive or negative? It should also capture the severity of the sentiment. The second part should contain the `relative volume` of the social media posts, as large volumes are more likely to be noticable. Part 2 will thus act as a way to strengen or weaken the total sentiment score of the day, by comparing the volume of that day with the average volume of the last 7 days. This part is worked out here: [part 2](#sub-bullet5.3). \n",
    "\n",
    "$\\text{Sentiment}_{t0} = \\text{Sentiment (part 1)}_{t0}*\\text{Sentiment relative volume (part 2)}_{t0}$\n",
    "\n",
    "**Part 1**\n",
    "The first part is the hardest one, as it is unclear what the best way to measure sentiment is. To tackle this problem, I will calculate the sentiment in different ways.\n",
    "- Method 1: positive / negative\n",
    "- <strike>Method 2: (positive - negative) / (positive + negative)</strike>\n",
    "- Method 3: Daily mean of compound sentiment score, counting only posts categorised as positive or negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd8ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_measures(return_df):\n",
    "    # Get results dataframe\n",
    "    sentiment_measures = count_posts(return_df)\n",
    "\n",
    "    # Only keep sentiment 2 results\n",
    "    sentiment_measures = sentiment_measures.loc[:, sentiment_measures.columns.str.contains('s1|s2|date')]\n",
    "    \n",
    "    # Method 1 - ratio\n",
    "    sentiment_measures['[s1]method_1'] = sentiment_measures['[s1]pos'] / sentiment_measures['[s1]total']\n",
    "    sentiment_measures['[s2]method_1'] = sentiment_measures['[s2]pos'] / sentiment_measures['[s2]total']\n",
    "\n",
    "    # Method 2 - discontinued as it is the same as method 1\n",
    "    #     sentiment_measures['[f1s2]method_2'] = (sentiment_measures['[f1s2]pos'] - sentiment_measures['[f1s2]neg']) / sentiment_measures['[f1s2]total']\n",
    "    #     sentiment_measures['[f2s2]method_2'] = (sentiment_measures['[f2s2]pos'] - sentiment_measures['[f2s2]neg']) / sentiment_measures['[f2s2]total']\n",
    "\n",
    "    # Method 3\n",
    "    to_merge = return_df[((return_df['s1_pos'] == 1) | (return_df['s1_neg'] == 1))][['date', 'compound_sent_1']].groupby('date').mean().rename(columns={'compound_sent_1': '[s1]method_3'})\n",
    "    sentiment_measures = sentiment_measures.merge(to_merge, how='left', left_on='date', right_on='date')\n",
    "    to_merge = return_df[((return_df['s2_pos'] == 1) | (return_df['s2_neg'] == 1))][['date', 'compound_sent_2']].groupby('date').mean().rename(columns={'compound_sent_2': '[s2]method_3'})\n",
    "\n",
    "    sentiment_measures = sentiment_measures.merge(to_merge, how='left', left_on='date', right_on='date')\n",
    "    return sentiment_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383edc92",
   "metadata": {},
   "source": [
    "**Actual sentiment calculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f5764d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AAPL] Done calculating sentiment after --- 82.22627329826355 seconds ---\n",
      "[AAPL] Done calculating sentiment after --- 27.112491369247437 seconds ---\n",
      "[AAPL] Done calculating sentiment after --- 106.12035155296326 seconds ---\n",
      "[AAPL] Done calculating sentiment after --- 40.65776085853577 seconds ---\n",
      "[AMD] Done calculating sentiment after --- 44.83538103103638 seconds ---\n",
      "[AMD] Done calculating sentiment after --- 23.831379413604736 seconds ---\n",
      "[AMD] Done calculating sentiment after --- 64.53612852096558 seconds ---\n",
      "[AMD] Done calculating sentiment after --- 38.3740599155426 seconds ---\n",
      "[AMZN] Done calculating sentiment after --- 82.37491154670715 seconds ---\n",
      "[AMZN] Done calculating sentiment after --- 27.920042753219604 seconds ---\n",
      "[AMZN] Done calculating sentiment after --- 106.65402173995972 seconds ---\n",
      "[AMZN] Done calculating sentiment after --- 40.22500658035278 seconds ---\n",
      "[ATVI] Done calculating sentiment after --- 0.8574912548065186 seconds ---\n",
      "[ATVI] Done calculating sentiment after --- 0.3877601623535156 seconds ---\n",
      "[ATVI] Done calculating sentiment after --- 2.8373587131500244 seconds ---\n",
      "[ATVI] Done calculating sentiment after --- 0.9294648170471191 seconds ---\n",
      "[BA] Done calculating sentiment after --- 18.37149739265442 seconds ---\n",
      "[BA] Done calculating sentiment after --- 10.907758951187134 seconds ---\n",
      "[BA] Done calculating sentiment after --- 27.897053956985474 seconds ---\n",
      "[BA] Done calculating sentiment after --- 16.156782865524292 seconds ---\n",
      "[BABA] Done calculating sentiment after --- 4.610363006591797 seconds ---\n",
      "[BABA] Done calculating sentiment after --- 1.7440154552459717 seconds ---\n",
      "[BABA] Done calculating sentiment after --- 8.096359252929688 seconds ---\n",
      "[BABA] Done calculating sentiment after --- 2.9943065643310547 seconds ---\n",
      "[BAC] Done calculating sentiment after --- 2.721461296081543 seconds ---\n",
      "[BAC] Done calculating sentiment after --- 0.9574582576751709 seconds ---\n",
      "[BAC] Done calculating sentiment after --- 4.466459274291992 seconds ---\n",
      "[BAC] Done calculating sentiment after --- 1.657050609588623 seconds ---\n",
      "[DIS] Done calculating sentiment after --- 29.974871158599854 seconds ---\n",
      "[DIS] Done calculating sentiment after --- 13.416325092315674 seconds ---\n",
      "[DIS] Done calculating sentiment after --- 43.465168714523315 seconds ---\n",
      "[DIS] Done calculating sentiment after --- 21.031980276107788 seconds ---\n",
      "[F] Done calculating sentiment after --- 71.19930267333984 seconds ---\n",
      "[F] Done calculating sentiment after --- 6.620214939117432 seconds ---\n",
      "[F] Done calculating sentiment after --- 95.52241253852844 seconds ---\n",
      "[F] Done calculating sentiment after --- 11.574358463287354 seconds ---\n",
      "[GE] Done calculating sentiment after --- 1.0584063529968262 seconds ---\n",
      "[GE] Done calculating sentiment after --- 0.19905805587768555 seconds ---\n",
      "[GE] Done calculating sentiment after --- 2.0038366317749023 seconds ---\n",
      "[GE] Done calculating sentiment after --- 0.49070072174072266 seconds ---\n",
      "[GME] Done calculating sentiment after --- 3.164205312728882 seconds ---\n",
      "[GME] Done calculating sentiment after --- 1.0893888473510742 seconds ---\n",
      "[GME] Done calculating sentiment after --- 4.546387672424316 seconds ---\n",
      "[GME] Done calculating sentiment after --- 1.9508872032165527 seconds ---\n",
      "[IQ] Done calculating sentiment after --- 0.5396769046783447 seconds ---\n",
      "[IQ] Done calculating sentiment after --- 0.1649036407470703 seconds ---\n",
      "[IQ] Done calculating sentiment after --- 2.082822322845459 seconds ---\n",
      "[IQ] Done calculating sentiment after --- 0.45372700691223145 seconds ---\n",
      "[LULU] Done calculating sentiment after --- 0.7625641822814941 seconds ---\n",
      "[LULU] Done calculating sentiment after --- 0.29982805252075195 seconds ---\n",
      "[LULU] Done calculating sentiment after --- 1.710022211074829 seconds ---\n",
      "[LULU] Done calculating sentiment after --- 0.6766064167022705 seconds ---\n",
      "[MSFT] Done calculating sentiment after --- 32.2117121219635 seconds ---\n",
      "[MSFT] Done calculating sentiment after --- 9.527539253234863 seconds ---\n",
      "[MSFT] Done calculating sentiment after --- 48.584229469299316 seconds ---\n",
      "[MSFT] Done calculating sentiment after --- 15.954879760742188 seconds ---\n",
      "[MU] Done calculating sentiment after --- 1.5850942134857178 seconds ---\n",
      "[MU] Done calculating sentiment after --- 0.999441385269165 seconds ---\n",
      "[MU] Done calculating sentiment after --- 3.1162192821502686 seconds ---\n",
      "[MU] Done calculating sentiment after --- 2.2247438430786133 seconds ---\n",
      "[NFLX] Done calculating sentiment after --- 29.611061334609985 seconds ---\n",
      "[NFLX] Done calculating sentiment after --- 10.33108639717102 seconds ---\n",
      "[NFLX] Done calculating sentiment after --- 41.35434818267822 seconds ---\n",
      "[NFLX] Done calculating sentiment after --- 18.337517976760864 seconds ---\n",
      "[NVDA] Done calculating sentiment after --- 10.249141693115234 seconds ---\n",
      "[NVDA] Done calculating sentiment after --- 2.0508251190185547 seconds ---\n",
      "[NVDA] Done calculating sentiment after --- 15.64405345916748 seconds ---\n",
      "[NVDA] Done calculating sentiment after --- 3.5839314460754395 seconds ---\n",
      "[SBUX] Done calculating sentiment after --- 6.022557497024536 seconds ---\n",
      "[SBUX] Done calculating sentiment after --- 2.8486077785491943 seconds ---\n",
      "[SBUX] Done calculating sentiment after --- 9.35265326499939 seconds ---\n",
      "[SBUX] Done calculating sentiment after --- 4.376642465591431 seconds ---\n",
      "[SHOP] Done calculating sentiment after --- 5.4678730964660645 seconds ---\n",
      "[SHOP] Done calculating sentiment after --- 2.029839038848877 seconds ---\n",
      "[SHOP] Done calculating sentiment after --- 8.965874671936035 seconds ---\n",
      "[SHOP] Done calculating sentiment after --- 3.191168785095215 seconds ---\n",
      "[SNAP] Done calculating sentiment after --- 4.297542572021484 seconds ---\n",
      "[SNAP] Done calculating sentiment after --- 2.115767240524292 seconds ---\n",
      "[SNAP] Done calculating sentiment after --- 8.93788766860962 seconds ---\n",
      "[SNAP] Done calculating sentiment after --- 4.9971325397491455 seconds ---\n",
      "[SQ] Done calculating sentiment after --- 25.721298456192017 seconds ---\n",
      "[SQ] Done calculating sentiment after --- 1.0484018325805664 seconds ---\n",
      "[SQ] Done calculating sentiment after --- 33.47386574745178 seconds ---\n",
      "[SQ] Done calculating sentiment after --- 1.955881118774414 seconds ---\n",
      "[TLRY] Done calculating sentiment after --- 1.736128330230713 seconds ---\n",
      "[TLRY] Done calculating sentiment after --- 0.981438398361206 seconds ---\n",
      "[TLRY] Done calculating sentiment after --- 3.2631328105926514 seconds ---\n",
      "[TLRY] Done calculating sentiment after --- 1.801952600479126 seconds ---\n",
      "[TSLA] Done calculating sentiment after --- 97.7401294708252 seconds ---\n",
      "[TSLA] Done calculating sentiment after --- 56.654629707336426 seconds ---\n",
      "[TSLA] Done calculating sentiment after --- 135.295804977417 seconds ---\n",
      "[TSLA] Done calculating sentiment after --- 83.62632298469543 seconds ---\n",
      "[V] Done calculating sentiment after --- 12.592803239822388 seconds ---\n",
      "[V] Done calculating sentiment after --- 3.305110216140747 seconds ---\n",
      "[V] Done calculating sentiment after --- 20.21244764328003 seconds ---\n",
      "[V] Done calculating sentiment after --- 5.774683713912964 seconds ---\n",
      "[WMT] Done calculating sentiment after --- 14.01198124885559 seconds ---\n",
      "[WMT] Done calculating sentiment after --- 4.8172290325164795 seconds ---\n",
      "[WMT] Done calculating sentiment after --- 19.85663628578186 seconds ---\n",
      "[WMT] Done calculating sentiment after --- 7.628638744354248 seconds ---\n"
     ]
    }
   ],
   "source": [
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\sentiment\\VADER\"\n",
    "method_1_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "method_2_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\method_2\"\n",
    "\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    def loop_func(comment_dir, filter_method=\"filter_1\", method_name=\"m1f1\"):\n",
    "        # ----------------------- m1f1/m1f2/m2f1/m2f2 -----------------------\n",
    "        # Setting up paths\n",
    "        comment_path = os.path.join(comment_dir, filter_method, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_path = os.path.join(save_dir, method_name, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        \n",
    "        # Check if file already exists and skip sentiment calculation if file exists\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"[skipping] File already exists: [{save_path}]\")\n",
    "\n",
    "        else: \n",
    "            # Reading csv's to df\n",
    "            comments_df = pd.read_csv(comment_path)\n",
    "\n",
    "            # Filter and clean data. Also perform VADER sentiment scoring.\n",
    "            return_df = clean_data(comments_df, ticker)\n",
    "\n",
    "            # Calculate sentiment scores for each method\n",
    "            sentiment_measures = calc_sent_measures(return_df)\n",
    "\n",
    "            # Save file\n",
    "            sentiment_measures.to_csv(save_path, encoding='utf-8', index=False)\n",
    "\n",
    "    # For each of the 4 method-filter combinations, run the the functions\n",
    "    loop_func(comment_dir=method_1_dir, filter_method=\"filter_1\",  method_name=\"m1f1\")\n",
    "    loop_func(comment_dir=method_1_dir, filter_method=\"filter_2\",  method_name=\"m1f2\")\n",
    "    loop_func(comment_dir=method_2_dir, filter_method=\"filter_1\",  method_name=\"m2f1\")\n",
    "    loop_func(comment_dir=method_2_dir, filter_method=\"filter_2\",  method_name=\"m2f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66221e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e4905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fafe65b0",
   "metadata": {},
   "source": [
    "### finBERT sentiment <a class=\"anchor\" id=\"sub-bullet5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6bb1c",
   "metadata": {},
   "source": [
    "**Setting up Pytorch and the finBERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28489294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbfc140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827c4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 750 Ti\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a44dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5633e",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1270003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runtime(function):\n",
    "    \"\"\"\n",
    "    A wrapper function that calculates the runtime of the specified function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = function(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Runtime for {function.__name__}: {end_time - start_time} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227483f7",
   "metadata": {},
   "source": [
    "The function below sets up the finBERT data pipeline and initializes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57876d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@calculate_runtime\n",
    "def calc_sent_finbert(df):\n",
    "    # Inititalise sentiment pipeline\n",
    "    nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # Create sentence list and run finBERT\n",
    "    sentence_list = df['body'].to_list()\n",
    "    results = nlp(sentence_list)\n",
    "\n",
    "    # Add results to main dataframe\n",
    "    results = pd.DataFrame(results)\n",
    "    df = df.merge(results, how='left', left_index=True, right_index=True)\n",
    "    return df  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f8dbb",
   "metadata": {},
   "source": [
    "**Deleted tables from comments** <a class=\"anchor\" id=\"delete-table-info\"></a>\n",
    "\n",
    "Although the data has already been cleaned, I discover that some posts contain tables. These tables consist out of many small characters and causes the finBERT model to be unable to process the comments. This is due to the 512 word limit the model has. \n",
    "\n",
    "This table cleaning is not done in the data seperation filtering part, as it can contain tickers and company names. It is however used in the VADER sentiment, which can be seen [here](#delete-table-info2).\n",
    "\n",
    "The regex pattern used to filter the tables is quite complex. I would recommend seeing what the regex pattern does using the following [website] (https://regex101.com/) and pasting the regex code together with a text sample.\n",
    "- regex code: `[^\\|\\s]{1,20}\\s*(?=\\|)|(?<=\\|)\\s*[^\\|]{1,30}\\s*(?=\\|)|(?<=\\|)\\s*[^\\|\\s]{1,30}`\n",
    "- text sample: `\"Company | Symbol | Price | Daily Change | 52W Change :-------|:------:|:------:|:------:|:------: General Electric Co | GE | 14.17 | -0.07% | -48.2% Tpi Composites Inc | TPIC | 26.76 | -0.04% | +61.3% [*]( 'Data from IEX')*[13-Week Price Moves](%) - [52 Week Price Change](%) - quote-bot by [echoapollo]()*\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e36658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company | Symbol | Price | Daily Change | 52W Change :-------|:------:|:------:|:------:|:------: General Electric Co | GE | 14.17 | -0.07% | -48.2% Tpi Composites Inc | TPIC | 26.76 | -0.04% | +61.3% [*]( 'Data from IEX')*[13-Week Price Moves](%) - [52 Week Price Change](%) - quote-bot by [echoapollo]()*\n",
      "\n",
      "-----> Turns into: ↓↓↓ \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" [*]( 'Data from IEX')*[13-Week Price Moves](%) - [52 Week Price Change](%) - quote-bot by [echoapollo]()*\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions borrowed from Twitter 'Data cleaning and sentiment' Jupyter Notebook\n",
    "def clean_text(text): \n",
    "    # Remove tables\n",
    "    text = re.sub(\"[^\\|\\s]{1,20}\\s*(?=\\|)|(?<=\\|)\\s*[^\\|]{1,30}\\s*(?=\\|)|(?<=\\|)\\s*[^\\|\\s]{1,30}\", \"\", text)\n",
    "    text = re.sub(\"\\|\", \"\", text)\n",
    "\n",
    "    \n",
    "    return text\n",
    "text = \"Company | Symbol | Price | Daily Change | 52W Change :-------|:------:|:------:|:------:|:------: General Electric Co | GE | 14.17 | -0.07% | -48.2% Tpi Composites Inc | TPIC | 26.76 | -0.04% | +61.3% [*]( 'Data from IEX')*[13-Week Price Moves](%) - [52 Week Price Change](%) - quote-bot by [echoapollo]()*\"\n",
    "print(text)\n",
    "print(\"\\n-----> Turns into: ↓↓↓ \\n\")\n",
    "clean_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7276ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finBERT_filter(df):\n",
    "    # Dropping any NA in body\n",
    "    df.dropna(subset=['body'], inplace=True)\n",
    "        \n",
    "    # Cleaning tables from posts\n",
    "    df['body'] = df['body'].apply(clean_text)\n",
    "\n",
    "    # Getting text lengths  \n",
    "    df['len'] = df['body'].str.len()\n",
    "\n",
    "    # Dropping texts above the 95% quantile\n",
    "    quantile_95 = df['len'].quantile(0.95)\n",
    "    df = df[df['len'] < quantile_95]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5b7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_in, ticker):\n",
    "    df = df_in.copy()\n",
    "    start_time = time.time()\n",
    "    # Prepping data by setting date column\n",
    "    df['created_at'] = pd.to_datetime(df['utc_datetime_str'])\n",
    "    df['date'] = df['created_at'].dt.date   \n",
    "    \n",
    "    # Renaming score to post_score (else conflict with finBERT results)\n",
    "    df.rename(columns={\"score\": \"post_score\"}, inplace=True)\n",
    "    \n",
    "    #   ---------------------------   NEW FILTERS   ---------------------------\n",
    "    # Filters out tables from post and deleted 5% largest posts\n",
    "    df = finBERT_filter(df)\n",
    "    \n",
    "    # Resetting index\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    #   ---------------------------   Sentiment   ---------------------------\n",
    "    df = calc_sent_finbert(df)\n",
    "    print(f\"[{ticker}] Done calculating sentiment after --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b0adc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_results(df):\n",
    "    # Drop scores with less than 80% certainty\n",
    "    df = df[df['score'] > 0.8]\n",
    "\n",
    "    # Drop all neutral observations\n",
    "    df = df[df['label'] != \"Neutral\"]\n",
    "    df['pos'] = np.where(df['label'] == \"Positive\", 1, 0)\n",
    "    df['neg'] = np.where(df['label'] == \"Negative\", 1, 0)\n",
    "\n",
    "    # Create results_df with [filter_1]\n",
    "    counted_results = df[['date', 'pos', 'neg']].groupby('date', as_index=False).sum().rename(columns={\"pos\": \"[BERT]pos\", \"neg\": \"[BERT]neg\"})\n",
    "    counted_results['[BERT]total'] = counted_results['[BERT]pos'] + counted_results['[BERT]neg']\n",
    "\n",
    "    return counted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3c529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_measures(return_df):\n",
    "    # Get results dataframe\n",
    "    sentiment_measures = count_results(return_df)\n",
    "    \n",
    "    # Method 1 - ratio\n",
    "    sentiment_measures['[BERT]method_1'] = sentiment_measures['[BERT]pos'] / sentiment_measures['[BERT]total']\n",
    "\n",
    "    # Method 2 - discontinued as it is the same as method 1\n",
    "    #     sentiment_measures['method_2'] = (sentiment_measures['pos'] - sentiment_measures['neg']) / sentiment_measures['total']\n",
    "\n",
    "    # Method 3\n",
    "    # Method 3 does not exist with finBERT\n",
    "\n",
    "    return sentiment_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30e24c",
   "metadata": {},
   "source": [
    "**Actual loop to calc finBERT sentiment for all files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de268a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skipping] File already exists: [E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/finBERT/m1f1/AAPL.csv]\n",
      "Runtime for calc_sent_finbert: 942.8509826660156 seconds\n",
      "[AAPL] Done calculating sentiment after --- 945.7433111667633 seconds ---\n",
      "Runtime for calc_sent_finbert: 4296.0922911167145 seconds\n",
      "[AAPL] Done calculating sentiment after --- 4308.085411071777 seconds ---\n",
      "Runtime for calc_sent_finbert: 1720.9407002925873 seconds\n",
      "[AAPL] Done calculating sentiment after --- 1725.547036409378 seconds ---\n",
      "Runtime for calc_sent_finbert: 2031.6958684921265 seconds\n",
      "[AMD] Done calculating sentiment after --- 2036.5930635929108 seconds ---\n",
      "Runtime for calc_sent_finbert: 1163.1408705711365 seconds\n",
      "[AMD] Done calculating sentiment after --- 1165.609448671341 seconds ---\n",
      "Runtime for calc_sent_finbert: 3803.923209667206 seconds\n",
      "[AMD] Done calculating sentiment after --- 3811.7896823883057 seconds ---\n",
      "Runtime for calc_sent_finbert: 2280.6406950950623 seconds\n",
      "[AMD] Done calculating sentiment after --- 2285.0551805496216 seconds ---\n",
      "Runtime for calc_sent_finbert: 2101.424391746521 seconds\n",
      "[AMZN] Done calculating sentiment after --- 2108.1795275211334 seconds ---\n",
      "Runtime for calc_sent_finbert: 802.0174033641815 seconds\n",
      "[AMZN] Done calculating sentiment after --- 804.4350244998932 seconds ---\n",
      "Runtime for calc_sent_finbert: 4078.547253847122 seconds\n",
      "[AMZN] Done calculating sentiment after --- 4089.657883644104 seconds ---\n",
      "Runtime for calc_sent_finbert: 1688.927565574646 seconds\n",
      "[AMZN] Done calculating sentiment after --- 1693.2987787723541 seconds ---\n",
      "Runtime for calc_sent_finbert: 30.104671716690063 seconds\n",
      "[ATVI] Done calculating sentiment after --- 30.300570964813232 seconds ---\n",
      "Runtime for calc_sent_finbert: 16.142661333084106 seconds\n",
      "[ATVI] Done calculating sentiment after --- 16.192635536193848 seconds ---\n",
      "Runtime for calc_sent_finbert: 148.31838250160217 seconds\n",
      "[ATVI] Done calculating sentiment after --- 148.75415587425232 seconds ---\n",
      "Runtime for calc_sent_finbert: 50.467928647994995 seconds\n",
      "[ATVI] Done calculating sentiment after --- 50.57887578010559 seconds ---\n",
      "Runtime for calc_sent_finbert: 695.0177516937256 seconds\n",
      "[BA] Done calculating sentiment after --- 696.9537720680237 seconds ---\n",
      "Runtime for calc_sent_finbert: 418.20437693595886 seconds\n",
      "[BA] Done calculating sentiment after --- 419.3149960041046 seconds ---\n",
      "Runtime for calc_sent_finbert: 1688.927565574646 seconds\n",
      "[AMZN] Done calculating sentiment after --- 1693.2987787723541 seconds ---\n",
      "Runtime for calc_sent_finbert: 30.104671716690063 seconds\n",
      "[ATVI] Done calculating sentiment after --- 30.300570964813232 seconds ---\n",
      "Runtime for calc_sent_finbert: 16.142661333084106 seconds\n",
      "[ATVI] Done calculating sentiment after --- 16.192635536193848 seconds ---\n",
      "Runtime for calc_sent_finbert: 148.31838250160217 seconds\n",
      "[ATVI] Done calculating sentiment after --- 148.75415587425232 seconds ---\n",
      "Runtime for calc_sent_finbert: 50.467928647994995 seconds\n",
      "[ATVI] Done calculating sentiment after --- 50.57887578010559 seconds ---\n",
      "Runtime for calc_sent_finbert: 695.0177516937256 seconds\n",
      "[BA] Done calculating sentiment after --- 696.9537720680237 seconds ---\n",
      "Runtime for calc_sent_finbert: 418.20437693595886 seconds\n",
      "[BA] Done calculating sentiment after --- 419.3149960041046 seconds ---\n",
      "Runtime for calc_sent_finbert: 1400.1020548343658 seconds\n",
      "[BA] Done calculating sentiment after --- 1403.559226989746 seconds ---\n",
      "Runtime for calc_sent_finbert: 808.8241033554077 seconds\n",
      "[BA] Done calculating sentiment after --- 810.8190145492554 seconds ---\n",
      "Runtime for calc_sent_finbert: 136.52814149856567 seconds\n",
      "[BABA] Done calculating sentiment after --- 137.02388954162598 seconds ---\n",
      "Runtime for calc_sent_finbert: 60.23543572425842 seconds\n",
      "[BABA] Done calculating sentiment after --- 60.421316385269165 seconds ---\n",
      "Runtime for calc_sent_finbert: 359.1111795902252 seconds\n",
      "[BABA] Done calculating sentiment after --- 360.150582075119 seconds ---\n",
      "Runtime for calc_sent_finbert: 129.60676670074463 seconds\n",
      "[BABA] Done calculating sentiment after --- 129.9665822982788 seconds ---\n",
      "Runtime for calc_sent_finbert: 70.75860118865967 seconds\n",
      "[BAC] Done calculating sentiment after --- 71.03244972229004 seconds ---\n",
      "Runtime for calc_sent_finbert: 31.759303331375122 seconds\n",
      "[BAC] Done calculating sentiment after --- 31.860259294509888 seconds ---\n",
      "Runtime for calc_sent_finbert: 187.1766917705536 seconds\n",
      "[BAC] Done calculating sentiment after --- 187.73636674880981 seconds ---\n",
      "Runtime for calc_sent_finbert: 74.04175806045532 seconds\n",
      "[BAC] Done calculating sentiment after --- 74.24164152145386 seconds ---\n",
      "Runtime for calc_sent_finbert: 1042.5069315433502 seconds\n",
      "[DIS] Done calculating sentiment after --- 1045.6242034435272 seconds ---\n",
      "Runtime for calc_sent_finbert: 538.1473588943481 seconds\n",
      "[DIS] Done calculating sentiment after --- 539.5205855369568 seconds ---\n",
      "[DIS] Done calculating sentiment after --- 2086.854485273361 seconds ---\n",
      "Runtime for calc_sent_finbert: 1134.4250812530518 seconds\n",
      "[DIS] Done calculating sentiment after --- 1137.0046172142029 seconds ---\n",
      "Runtime for calc_sent_finbert: 1786.8452384471893 seconds\n",
      "[F] Done calculating sentiment after --- 1793.0277390480042 seconds ---\n",
      "Runtime for calc_sent_finbert: 262.486323595047 seconds\n",
      "[F] Done calculating sentiment after --- 263.21092557907104 seconds ---\n",
      "[F] Done calculating sentiment after --- 3273.819652080536 seconds ---\n",
      "Runtime for calc_sent_finbert: 541.8088941574097 seconds\n",
      "[F] Done calculating sentiment after --- 543.1471312046051 seconds ---\n",
      "Runtime for calc_sent_finbert: 34.5882465839386 seconds\n",
      "[GE] Done calculating sentiment after --- 34.72516441345215 seconds ---\n",
      "Runtime for calc_sent_finbert: 8.822959899902344 seconds\n",
      "[GE] Done calculating sentiment after --- 8.85595440864563 seconds ---\n",
      "Runtime for calc_sent_finbert: 92.79001808166504 seconds\n",
      "[GE] Done calculating sentiment after --- 93.0818383693695 seconds ---\n",
      "Runtime for calc_sent_finbert: 26.37295436859131 seconds\n",
      "[GE] Done calculating sentiment after --- 26.44391393661499 seconds ---\n",
      "Runtime for calc_sent_finbert: 107.8763780593872 seconds\n",
      "[GME] Done calculating sentiment after --- 108.20547699928284 seconds ---\n",
      "Runtime for calc_sent_finbert: 41.399352073669434 seconds\n",
      "[GME] Done calculating sentiment after --- 41.51626753807068 seconds ---\n",
      "Runtime for calc_sent_finbert: 217.13996696472168 seconds\n",
      "[GME] Done calculating sentiment after --- 217.74362134933472 seconds ---\n",
      "Runtime for calc_sent_finbert: 94.24216032028198 seconds\n",
      "[GME] Done calculating sentiment after --- 94.4750485420227 seconds ---\n",
      "Runtime for calc_sent_finbert: 21.360782861709595 seconds\n",
      "[IQ] Done calculating sentiment after --- 21.439751625061035 seconds ---\n",
      "Runtime for calc_sent_finbert: 8.790972471237183 seconds\n",
      "[IQ] Done calculating sentiment after --- 8.814977169036865 seconds ---\n",
      "Runtime for calc_sent_finbert: 115.40206694602966 seconds\n",
      "[IQ] Done calculating sentiment after --- 115.70811080932617 seconds ---\n",
      "Runtime for calc_sent_finbert: 26.870661735534668 seconds\n",
      "[IQ] Done calculating sentiment after --- 26.931607961654663 seconds ---\n",
      "Runtime for calc_sent_finbert: 32.270557165145874 seconds\n",
      "[LULU] Done calculating sentiment after --- 32.36351418495178 seconds ---\n",
      "Runtime for calc_sent_finbert: 14.060960292816162 seconds\n",
      "[LULU] Done calculating sentiment after --- 14.09694790840149 seconds ---\n",
      "Runtime for calc_sent_finbert: 89.08010601997375 seconds\n",
      "[LULU] Done calculating sentiment after --- 89.29598426818848 seconds ---\n",
      "Runtime for calc_sent_finbert: 39.22278332710266 seconds\n",
      "[LULU] Done calculating sentiment after --- 39.314711570739746 seconds ---\n",
      "Runtime for calc_sent_finbert: 905.692539691925 seconds\n",
      "[MSFT] Done calculating sentiment after --- 908.6348555088043 seconds ---\n",
      "Runtime for calc_sent_finbert: 362.67277002334595 seconds\n",
      "[MSFT] Done calculating sentiment after --- 363.6691966056824 seconds ---\n",
      "Runtime for calc_sent_finbert: 1940.7728328704834 seconds\n",
      "[MSFT] Done calculating sentiment after --- 1946.2117002010345 seconds ---\n",
      "Runtime for calc_sent_finbert: 778.0732352733612 seconds\n",
      "[MSFT] Done calculating sentiment after --- 779.9821448326111 seconds ---\n",
      "Runtime for calc_sent_finbert: 88.36348557472229 seconds\n",
      "[MU] Done calculating sentiment after --- 88.55239033699036 seconds ---\n",
      "Runtime for calc_sent_finbert: 66.24013876914978 seconds\n",
      "[MU] Done calculating sentiment after --- 66.36206817626953 seconds ---\n",
      "Runtime for calc_sent_finbert: 206.8769724369049 seconds\n",
      "[MU] Done calculating sentiment after --- 207.29072284698486 seconds ---\n",
      "Runtime for calc_sent_finbert: 161.6256320476532 seconds\n",
      "[MU] Done calculating sentiment after --- 161.91145157814026 seconds ---\n",
      "Runtime for calc_sent_finbert: 839.1834321022034 seconds\n",
      "[NFLX] Done calculating sentiment after --- 841.8149292469025 seconds ---\n",
      "Runtime for calc_sent_finbert: 366.11775946617126 seconds\n",
      "[NFLX] Done calculating sentiment after --- 367.153169631958 seconds ---\n",
      "Runtime for calc_sent_finbert: 1507.5519495010376 seconds\n",
      "[NFLX] Done calculating sentiment after --- 1511.874496936798 seconds ---\n",
      "Runtime for calc_sent_finbert: 697.0107100009918 seconds\n",
      "[NFLX] Done calculating sentiment after --- 698.8856360912323 seconds ---\n",
      "Runtime for calc_sent_finbert: 298.1955499649048 seconds\n",
      "[NVDA] Done calculating sentiment after --- 299.12703371047974 seconds ---\n",
      "Runtime for calc_sent_finbert: 77.61062288284302 seconds\n",
      "[NVDA] Done calculating sentiment after --- 77.81350111961365 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\sentiment\\finBERT\"\n",
    "method_1_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "method_2_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\method_2\"\n",
    "\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    def loop_func(comment_dir, filter_method=\"filter_1\", method_name=\"m1f1\"):\n",
    "        # ----------------------- m1f1/m1f2/m2f1/m2f2 -----------------------\n",
    "        # Creating comment_path and save_path\n",
    "        comment_path = os.path.join(comment_dir, filter_method, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_path = os.path.join(save_dir, method_name, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Check if file already exists and skip sentiment calculation if file exists\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"[skipping] File already exists: [{save_path}]\")\n",
    "\n",
    "        else:\n",
    "            # Reading csv's to df\n",
    "            comments_df = pd.read_csv(comment_path)\n",
    "\n",
    "            # Filter and clean data. Also perform VADER sentiment scoring.\n",
    "            return_df = clean_data(comments_df, ticker)\n",
    "\n",
    "            # Calculate sentiment scores for each method\n",
    "            sentiment_measures = calc_sent_measures(return_df)\n",
    "\n",
    "            #Save file\n",
    "            sentiment_measures.to_csv(save_path, encoding='utf-8', index=False)\n",
    "\n",
    "    # For each of the 4 method-filter combinations, run the the functions\n",
    "    loop_func(comment_dir=method_1_dir, filter_method=\"filter_1\",  method_name=\"m1f1\")\n",
    "    loop_func(comment_dir=method_1_dir, filter_method=\"filter_2\",  method_name=\"m1f2\")\n",
    "    loop_func(comment_dir=method_2_dir, filter_method=\"filter_1\",  method_name=\"m2f1\")\n",
    "    loop_func(comment_dir=method_2_dir, filter_method=\"filter_2\",  method_name=\"m2f2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc6896",
   "metadata": {},
   "source": [
    "### Relative volume and final sentiment <a class=\"anchor\" id=\"sub-bullet5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a8052",
   "metadata": {},
   "source": [
    "As explained [here](#sentiment-measure), the sentiment measure consists out of two parts. The second part is now discussed.\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "The second part is the easier one of the two parts to measure. With the sentiment tools all returning either a positive, negative or neutral label, I decide that I will solely be focussing on the positive and negative posts. This means that these are also the posts that will be taken into considerations when looking at volume. The volume will be measured by counting the total of positive and negative posts for a given day. This daily total will then be divided by the average total posts of the last 7 days.\n",
    "\n",
    "- $\\text{Relative volume}_{t0} = \\frac{\\text{Total positive posts}_{t0}+\\text{Total negative posts}_{t0}}{Rolling mean 7(\\text{Total positive posts}_{t0}+\\text{Total negative posts}_{t0})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a139d",
   "metadata": {},
   "source": [
    "Calculating the final sentiment is done using multiplying the relative volume times the sentiment.\n",
    "\n",
    "- $\\text{Sentiment}_{t0} = \\text{Sentiment (part 1)}_{t0}*\\text{Sentiment relative volume (part 2)}_{t0}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3355076",
   "metadata": {},
   "source": [
    "**VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e38588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_vol(df):\n",
    "    df['[s1]rel_vol'] = df['[s1]total'] / df['[s1]total'].rolling(7).mean()\n",
    "    df['[s2]rel_vol'] = df['[s2]total'] / df['[s2]total'].rolling(7).mean()\n",
    "\n",
    "    # Dropping NA values\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d30b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_final_sent(df):\n",
    "    df['[s1M1]final_sent'] = df['[s1]method_1'] * df['[s1]rel_vol']\n",
    "    df['[s1M3]final_sent'] = df['[s1]method_3'] * df['[s1]rel_vol']\n",
    "    df['[s2M1]final_sent'] = df['[s2]method_1'] * df['[s2]rel_vol']\n",
    "    df['[s2M3]final_sent'] = df['[s2]method_3'] * df['[s2]rel_vol']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ffb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\sentiment\\VADER\"\n",
    "\n",
    "save = False\n",
    "\n",
    "if save:\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Calculating relative volume and final sentiment\n",
    "            df = calc_relative_vol(df)\n",
    "            df = calc_final_sent(df)\n",
    "\n",
    "\n",
    "            #Save file\n",
    "            df.to_csv(csv_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c3aca",
   "metadata": {},
   "source": [
    "**finBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2782662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_vol(df):\n",
    "    df['[BERT]rel_vol'] = df['[BERT]total'] / df['[BERT]total'].rolling(7).mean()\n",
    "\n",
    "    # Dropping NA values\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5c830e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_final_sent(df):\n",
    "    df['[BERT]final_sent'] = df['[BERT]method_1'] * df['[BERT]rel_vol']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce01dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\sentiment\\finBERT\"\n",
    "\n",
    "save = False\n",
    "\n",
    "if save:\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Calculating relative volume and final sentiment\n",
    "            df = calc_relative_vol(df)\n",
    "            df = calc_final_sent(df)\n",
    "\n",
    "            #Save file\n",
    "            df.to_csv(csv_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc329dbb",
   "metadata": {},
   "source": [
    "## ToDo <a class=\"anchor\" id=\"ToDo\"></a>\n",
    "\n",
    "[Go back up](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d750848",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>[BERT]pos</th>\n",
       "      <th>[BERT]neg</th>\n",
       "      <th>[BERT]total</th>\n",
       "      <th>[BERT]method_1</th>\n",
       "      <th>[BERT]rel_vol</th>\n",
       "      <th>[BERT]final_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-05-13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>0.225806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>2.145161</td>\n",
       "      <td>1.129032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-05-17</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.032258</td>\n",
       "      <td>1.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>2020-08-27</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>44</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>1.088339</td>\n",
       "      <td>0.469965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.953405</td>\n",
       "      <td>0.577061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.340824</td>\n",
       "      <td>0.262172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.248031</td>\n",
       "      <td>0.192913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>71</td>\n",
       "      <td>24</td>\n",
       "      <td>95</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>2.152104</td>\n",
       "      <td>1.608414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>833 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  [BERT]pos  [BERT]neg  [BERT]total  [BERT]method_1  \\\n",
       "6    2018-05-13          4          2            6        0.666667   \n",
       "7    2018-05-14          2          1            3        0.666667   \n",
       "8    2018-05-15          3          2            5        0.600000   \n",
       "9    2018-05-16         10          9           19        0.526316   \n",
       "10   2018-05-17         12          6           18        0.666667   \n",
       "..          ...        ...        ...          ...             ...   \n",
       "834  2020-08-27         19         25           44        0.431818   \n",
       "835  2020-08-28         23         15           38        0.605263   \n",
       "836  2020-08-29         10          3           13        0.769231   \n",
       "837  2020-08-30          7          2            9        0.777778   \n",
       "838  2020-08-31         71         24           95        0.747368   \n",
       "\n",
       "     [BERT]rel_vol  [BERT]final_sent  \n",
       "6         0.636364          0.424242  \n",
       "7         0.338710          0.225806  \n",
       "8         0.625000          0.375000  \n",
       "9         2.145161          1.129032  \n",
       "10        2.032258          1.354839  \n",
       "..             ...               ...  \n",
       "834       1.088339          0.469965  \n",
       "835       0.953405          0.577061  \n",
       "836       0.340824          0.262172  \n",
       "837       0.248031          0.192913  \n",
       "838       2.152104          1.608414  \n",
       "\n",
       "[833 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\sentiment\\finBERT\\m1f1\\AMD.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = calc_relative_vol(df)\n",
    "df = calc_final_sent(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8c4cfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\\filter_1\\MU.csv\"\n",
    "comments_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c77b6f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for calc_sent_finbert: 107.86515927314758 seconds\n"
     ]
    }
   ],
   "source": [
    "ticker = \"MU\"\n",
    "return_df = clean_data(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96e88021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['body'].str.len()\n",
    "df['keep'] = (df['len'] / 512 < 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe1e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a84ecd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ATVI\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9548\\851111397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mticker_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing: {ticker.ljust(4)}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "import time\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    print(f\"Processing: {ticker.ljust(4)}\", end='\\r')\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1072017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
