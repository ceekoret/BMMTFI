{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8e5854",
   "metadata": {},
   "source": [
    "# Reddit sentiment <a class=\"anchor\" id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c26d1",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Reddit filters and sentiment methods](#bullet1)\n",
    "* [General functions and imports](#bullet2)\n",
    "    - [Filter methods](#sub-bullet2.1)\n",
    "* [Reddit method 1](#bullet3)\n",
    "* [Reddit method 2](#bullet4)\n",
    "* [Sentiment calculations](#bullet5)\n",
    "    - [VADER sentiment](#sub-bullet5.1)\n",
    "    - [finBERT sentiment](#sub-bullet5.2)\n",
    "* [ToDo](#ToDo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404b26c",
   "metadata": {},
   "source": [
    "## Reddit filters and sentiment methods <a class=\"anchor\" id=\"bullet1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61d0ce",
   "metadata": {},
   "source": [
    "Reddit sentiment is calculated in different ways. I use 2 different 'Reddit-methods' and 2 different [relevancy filters](#sub-bullet2.1) to approach the Reddit data. \n",
    "\n",
    "| Filter | Method 1 | Method 2 |\n",
    "| --- | --- | --- |\n",
    "| Filter 1 | m1f1 | m2f1 |\n",
    "| Filter 2 | m1f2 | m2f2 |\n",
    "\n",
    "\n",
    "On these four combinations, two different sentiment analysis methods will be performed.\n",
    "\n",
    "\n",
    "**Reddit methods**\n",
    "- The first way is similar to the method used for Twitter posts. All Reddit comments will be scanned and filtered for stock mentions. These stock mentions are then counted for each stock and subsequent sentiment analysis will be performed on them.\n",
    "- The second method will not only look at comments, but also consider the Reddit posts. This is done in line with [Hu, Jones, Zhang and Zhang (2021) page 10-12](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3807655). The method begins start by checking whether a post or comment contains a company ticker or name. Once such a mention is found, any direct reply's are searched. All posts or comments mentioning a company together with any direct reply's to these posts or comments are counted as social media activity for this particular company. \n",
    "\n",
    "These methods differ slightly from the Reddit comments. The reason for this is that it seems that the `$ticker` naming convention does not seem popular on Reddit. Searching only for `$tsla` returns only 58 observations for the r/stocks subreddit during the 2020_07 period. When searching for the company name instead (Tesla), 4192 observations are found. Hence Reddit comments will also be searched using company names.\n",
    "\n",
    "\n",
    "A total of `20.511.418` comments are scraped from the 5 subreddits investing, pennystocks, stockmarket, stocks and wallstreetbets. `844.715` comments posts are found for method 1. Method 2 ... `36.244` posts\n",
    "\n",
    "**Sentiment methods**\n",
    "The sentiment methods which are used are:\n",
    "- cjhutto VADER sentiment\n",
    "- finBERT sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468761a2",
   "metadata": {},
   "source": [
    "## General functions and imports <a class=\"anchor\" id=\"bullet2\"></a>\n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf75eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import statsmodels.formula.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb07310",
   "metadata": {},
   "source": [
    "**General functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af720a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GameStop'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of company names\n",
    "company_names = {'TSLA': 'Tesla', \n",
    "                 'MU': 'Micron Technology', \n",
    "                 'SNAP': 'Snapchat', \n",
    "                 'AMD': 'AMD', \n",
    "                 'DIS': 'Disney', \n",
    "                 'MSFT': 'Microsoft', \n",
    "                 'AAPL': 'Apple', \n",
    "                 'AMZN': 'Amazon', \n",
    "                 'SQ': 'Block', \n",
    "                 'BABA': 'Alibaba', \n",
    "                 'V': 'Visa', \n",
    "                 'NFLX': 'Netflix', \n",
    "                 'IQ': 'IQIYI', \n",
    "                 'ATVI': 'Activision Blizzard', \n",
    "                 'SHOP': 'Shopify', \n",
    "                 'BA': 'Boeing', \n",
    "                 'NVDA': 'NVIDIA', \n",
    "                 'GE': 'General Electric', \n",
    "                 'WMT': 'Walmart', \n",
    "                 'SBUX': 'Starbucks', \n",
    "                 'F': 'Ford', \n",
    "                 'TLRY': 'Tilray', \n",
    "                 'LULU': 'Lululemon', \n",
    "                 'BAC': 'Bank of America', \n",
    "                 'GME': 'GameStop'}\n",
    "\n",
    "# Return name of company name dict\n",
    "def find_company_name(ticker):\n",
    "    ticker = ticker.upper()\n",
    "    \n",
    "    return company_names[ticker]\n",
    "    \n",
    "    \n",
    "find_company_name(\"gme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5463b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of top NASDAQ and S&P companies\n",
    "company_list = ['MetLife', 'Exelon', \"O'Reilly Automotive\", 'Baker Hughes', 'Rivian',\n",
    "                'Applied Materials', 'Palo Alto Networks', 'Alphabet', 'Nike',\n",
    "                'ASML', 'Lululemon', 'Bank of America', 'Cadence Design Systems',\n",
    "                'BNY Mellon', 'Pfizer', 'Cintas', 'Google', 'Marriott International', 'American Tower',\n",
    "                'Thermo Fisher Scientific', 'CrowdStrike', 'Ross Stores', 'Emerson', 'Lilly', 'Linde',\n",
    "                'Salesforce', 'Qualcomm', 'CoStar Group', 'T-Mobile','TMobile', 'Caterpillar', 'Cognizant',\n",
    "                'FedEx', 'JD.com', 'Johnson & Johnson', 'Alibaba', 'Netflix', 'Seagen', 'Sirius XM',\n",
    "                'Procter & Gamble', 'Microchip Technology', 'PepsiCo', 'Nvidia', 'Dow', 'Zscaler',\n",
    "                'Abbott', 'Charter Communications', 'Charles Schwab', \"McDonald's\", 'Mastercard',\n",
    "                'Ford', 'MercadoLibre', 'Booking Holdings', 'Diamondback Energy', 'Dollar Tree',\n",
    "                'Verisk', 'Zoom Video Communications', 'Altria', 'IQIYI', 'Constellation Energy',\n",
    "                'Wells Fargo', 'Starbucks', 'NXP', 'Adobe', 'eBay', 'Gilead', 'American Electric Power',\n",
    "                'Lucid Motors', 'Disney', 'Coca-Cola', 'Visa', 'Illumina, Inc.', 'Moderna', 'Fiserv',\n",
    "                'Boeing', 'Medtronic', 'Gilead Sciences', 'GlobalFoundries', 'NextEra Energy',\n",
    "                'Texas Instruments', 'Intel', 'Monster Beverage', 'Block', 'Meta Platforms', 'Tesla',\n",
    "                'KLA Corporation', 'Broadcom', 'Airbnb', 'PayPal', 'CVS Health', 'AT&T', 'Cisco',\n",
    "                'Raytheon Technologies', 'U.S. Bank', 'Duke Energy', 'Union Pacific', '3M',\n",
    "                'Lockheed Martin', 'Ansys', 'ExxonMobil', 'Verizon', 'Xcel Energy', 'Paccar', 'Costco',\n",
    "                'Analog Devices', 'AbbVie', 'Lam Research', 'Vertex Pharmaceuticals', 'Intuitive Surgical',\n",
    "                'Synopsys', 'AMD', 'American Express', 'Regeneron', 'Activision Blizzard', 'Target',\n",
    "                'Atlassian', 'Advanced Micro Devices', 'Warner Bros. Discovery', 'Tilray', 'BlackRock',\n",
    "                'Amazon', 'Walmart', 'Workday', 'Dr Pepper', 'JPMorgan Chase', 'Meta', 'Copart',\n",
    "                'Biogen', 'Paychex', 'Capital One', 'UnitedHealth Group', 'Fastenal', 'Microsoft',\n",
    "                'AstraZeneca', \"Lowe's\", 'Datadog', 'General Electric', 'ADP', 'Goldman Sachs', 'PDD Holdings',\n",
    "                'Accenture', 'Mondelēz International', 'United Parcel Service', 'Comcast', 'Micron Technology',\n",
    "                'CSX Corporation', 'Electronic Arts', 'Marvell Technology', 'ConocoPhillips',\n",
    "                'Walgreens Boots Alliance', 'Home Depot', 'Idexx Laboratories', 'IBM', 'Oracle', 'Shopify',\n",
    "                'Apple', 'GE', 'Morgan Stanley', 'Merck', 'Citigroup', 'Simon', 'Amgen',\n",
    "                'Philip Morris International', 'General Dynamics', 'Bristol Myers Squibb', \n",
    "                'Old Dominion Freight Line', 'Snapchat', 'GameStop', 'Align Technology', 'Colgate-Palmolive', \n",
    "                'Berkshire Hathaway', 'Enphase Energy', 'Fortinet', 'Intuit', 'Danaher', 'Kraft Heinz', 'Chevron', \n",
    "                'GM', 'DexCom', 'Southern Company', 'Autodesk', 'American International Group', 'Honeywell']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63867ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"\\bMetLife\\b|\\bExelon\\b|\\bO'Reilly Automotive\\b|\\bBaker Hughes\\b|\\bRivian\\b|\\bApplied Materials\\b|\\bPalo Alto Networks\\b|\\bAlphabet\\b|\\bNike\\b|\\bASML\\b|\\bLululemon\\b|\\bBank of America\\b|\\bCadence Design Systems\\b|\\bBNY Mellon\\b|\\bPfizer\\b|\\bCintas\\b|\\bGoogle\\b|\\bMarriott International\\b|\\bAmerican Tower\\b|\\bThermo Fisher Scientific\\b|\\bCrowdStrike\\b|\\bRoss Stores\\b|\\bEmerson\\b|\\bLilly\\b|\\bLinde\\b|\\bSalesforce\\b|\\bQualcomm\\b|\\bCoStar Group\\b|\\bT-Mobile\\b|\\bTMobile\\b|\\bCaterpillar\\b|\\bCognizant\\b|\\bFedEx\\b|\\bJD.com\\b|\\bJohnson & Johnson\\b|\\bAlibaba\\b|\\bNetflix\\b|\\bSeagen\\b|\\bSirius XM\\b|\\bProcter & Gamble\\b|\\bMicrochip Technology\\b|\\bPepsiCo\\b|\\bNvidia\\b|\\bDow\\b|\\bZscaler\\b|\\bAbbott\\b|\\bCharter Communications\\b|\\bCharles Schwab\\b|\\bMcDonald's\\b|\\bMastercard\\b|\\bFord\\b|\\bMercadoLibre\\b|\\bBooking Holdings\\b|\\bDiamondback Energy\\b|\\bDollar Tree\\b|\\bVerisk\\b|\\bZoom Video Communications\\b|\\bAltria\\b|\\bIQIYI\\b|\\bConstellation Energy\\b|\\bWells Fargo\\b|\\bStarbucks\\b|\\bNXP\\b|\\bAdobe\\b|\\beBay\\b|\\bGilead\\b|\\bAmerican Electric Power\\b|\\bLucid Motors\\b|\\bDisney\\b|\\bCoca-Cola\\b|\\bVisa\\b|\\bIllumina, Inc.\\b|\\bModerna\\b|\\bFiserv\\b|\\bBoeing\\b|\\bMedtronic\\b|\\bGilead Sciences\\b|\\bGlobalFoundries\\b|\\bNextEra Energy\\b|\\bTexas Instruments\\b|\\bIntel\\b|\\bMonster Beverage\\b|\\bBlock\\b|\\bMeta Platforms\\b|\\bTesla\\b|\\bKLA Corporation\\b|\\bBroadcom\\b|\\bAirbnb\\b|\\bPayPal\\b|\\bCVS Health\\b|\\bAT&T\\b|\\bCisco\\b|\\bRaytheon Technologies\\b|\\bU.S. Bank\\b|\\bDuke Energy\\b|\\bUnion Pacific\\b|\\b3M\\b|\\bLockheed Martin\\b|\\bAnsys\\b|\\bExxonMobil\\b|\\bVerizon\\b|\\bXcel Energy\\b|\\bPaccar\\b|\\bCostco\\b|\\bAnalog Devices\\b|\\bAbbVie\\b|\\bLam Research\\b|\\bVertex Pharmaceuticals\\b|\\bIntuitive Surgical\\b|\\bSynopsys\\b|\\bAMD\\b|\\bAmerican Express\\b|\\bRegeneron\\b|\\bActivision Blizzard\\b|\\bTarget\\b|\\bAtlassian\\b|\\bAdvanced Micro Devices\\b|\\bWarner Bros. Discovery\\b|\\bTilray\\b|\\bBlackRock\\b|\\bAmazon\\b|\\bWalmart\\b|\\bWorkday\\b|\\bDr Pepper\\b|\\bJPMorgan Chase\\b|\\bMeta\\b|\\bCopart\\b|\\bBiogen\\b|\\bPaychex\\b|\\bCapital One\\b|\\bUnitedHealth Group\\b|\\bFastenal\\b|\\bMicrosoft\\b|\\bAstraZeneca\\b|\\bLowe's\\b|\\bDatadog\\b|\\bGeneral Electric\\b|\\bADP\\b|\\bGoldman Sachs\\b|\\bPDD Holdings\\b|\\bAccenture\\b|\\bMondelēz International\\b|\\bUnited Parcel Service\\b|\\bComcast\\b|\\bMicron Technology\\b|\\bCSX Corporation\\b|\\bElectronic Arts\\b|\\bMarvell Technology\\b|\\bConocoPhillips\\b|\\bWalgreens Boots Alliance\\b|\\bHome Depot\\b|\\bIdexx Laboratories\\b|\\bIBM\\b|\\bOracle\\b|\\bShopify\\b|\\bApple\\b|\\bGE\\b|\\bMorgan Stanley\\b|\\bMerck\\b|\\bCitigroup\\b|\\bSimon\\b|\\bAmgen\\b|\\bPhilip Morris International\\b|\\bGeneral Dynamics\\b|\\bBristol Myers Squibb\\b|\\bOld Dominion Freight Line\\b|\\bSnapchat\\b|\\bGameStop\\b|\\bAlign Technology\\b|\\bColgate-Palmolive\\b|\\bBerkshire Hathaway\\b|\\bEnphase Energy\\b|\\bFortinet\\b|\\bIntuit\\b|\\bDanaher\\b|\\bKraft Heinz\\b|\\bChevron\\b|\\bGM\\b|\\bDexCom\\b|\\bSouthern Company\\b|\\bAutodesk\\b|\\bAmerican International Group\\b|\\bHoneywell\\b\",\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('|'.join(['|'.join([f\"\\\\b{name}\\\\b\" for name in company_list])]))\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d81e38",
   "metadata": {},
   "source": [
    "### Filter methods <a class=\"anchor\" id=\"sub-bullet2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe8c32",
   "metadata": {},
   "source": [
    "Besides the two [Reddit-methods](#bullet1) which were described in the introduction, I also make use of two different filter. These filters are used to check if a Tweet is relevant to a specific company.\n",
    "\n",
    "To summarize:\n",
    "- `filter_data_1` Checks whether the name of the company or the ticker of the company is mentioned. If one of these is true, the comment or post is seen as relevant.\n",
    "- `filter_data_2` Is more strict that filter_data_1. Just like the filter 1, is checks for the company name and ticker. However, this time it also checks whether other tickers or company names are mentioned. It does this using NASDAQ and S&P company names which are summarized in `company_list`. These company names are then compiled into a regex search. If it turns out that different tickers or company names are mentioned in the post or comment, the post or comment is not considered relevant. If only the company ticker or company's name is mentioned, the post <u>is</u> considered relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e9c2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Functions borrowed from Twitter 'Data cleaning and sentiment' Jupyter Notebook\n",
    "def clean_text(text):\n",
    "    # Remove twitter Return handles (RT @xxx:)\n",
    "    text = re.sub(\"RT @[\\w]*:\", \"\", text)\n",
    "\n",
    "    # Remove twitter handles (@xxx)\n",
    "    text = re.sub(\"@[\\w]*\", \"\", text)\n",
    "\n",
    "    # Remove URL links (httpxxx)\n",
    "    url_matcher = \"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "    text = re.sub(url_matcher, \"\", text)\n",
    "    \n",
    "    # Remove any multiple white spaces, tabs or newlines\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    \n",
    "    #remove “”\n",
    "    text = re.sub(\"“|”\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# This function is slightly adjusted. Not only are company tickers searched (\"$ticker\"), but also company names are searched.\n",
    "# This change increases the amount of mentions for Tesla on the r/stocks subreddit during the 2020_07 period from 58 to 4192 observations\n",
    "def filter_data_1(post, ticker):\n",
    "    # Filter out posts that do not mention the company ticker.\n",
    "    company_name = find_company_name(ticker)\n",
    "    if bool(re.search(fr\"(\\${ticker})|({company_name})\", post, re.IGNORECASE)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Method 2 filters the comments based on the rule that exactly 1 ticker or company name is mentioned and ...\n",
    "# ... that this ticker is the ticker of the company of which the sentiment is being calculated   \n",
    "def filter_data_2(post, ticker):\n",
    "    # ---- Tickers ----\n",
    "    # Count the number of tickers in the post\n",
    "    ticker_matches = len(re.findall(r\"\\$[a-zA-Z]+\", post, re.IGNORECASE))\n",
    "    company_ticker_matches = len(re.findall(fr\"\\${ticker}\", post, re.IGNORECASE))\n",
    "    \n",
    "    # The ticker_diff needs to be equal to zero, else other tickers than the company ticker are mentioned.\n",
    "    ticker_diff = ticker_matches - company_ticker_matches\n",
    "    \n",
    "    # print(f\"{len(ticker_matches)} - {len(company_ticker_matches)} = {ticker_diff}\")\n",
    "    \n",
    "    # ---- Company name ----\n",
    "    # Create pattern which matches any of the company names in the company_list\n",
    "    # '\\\\b' prevents any occurences of short company names from being picked up mid-string:\n",
    "    # GE --> 'Vegetable' would be picked up without this rule\n",
    "    pattern = re.compile('|'.join(['|'.join([f\"\\\\b{name}\\\\b\" for name in company_list])]))\n",
    "    name_matches = len(pattern.findall(post, re.IGNORECASE))\n",
    "    \n",
    "    # Next the company of the ticker is searched.\n",
    "    company_name = find_company_name(ticker)\n",
    "    company_name_matches = len(re.findall(f\"\\\\b{company_name}\\\\b\", post, re.IGNORECASE))\n",
    "    \n",
    "    # The company_name_diff needs to be equal to zero, else other tickers than the company ticker are mentioned.\n",
    "    company_name_diff = name_matches - company_name_matches\n",
    "    \n",
    "    # print(f\"{name_matches_count} - {len(company_name_matches)} = {company_name_diff}\")\n",
    "    \n",
    "    # Filter out posts with more or less than 1 ticker, and check whether this 1 ticker is the company ticker.\n",
    "    if ticker_diff == 0 and company_name_diff == 0 and (company_ticker_matches > 0 or company_name_matches > 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "text = \"This is a test text. If Apple is in the text, without any other companies mentioned, the functions returns True.\"\n",
    "print(filter_data_2(text, \"aapl\"))\n",
    "\n",
    "text = \"If another company is mentioned, like Gamestop, it returns False\"\n",
    "print(filter_data_2(text, \"aapl\"))\n",
    "\n",
    "text = \"If no company is mentioned, it also returns False\"\n",
    "print(filter_data_2(text, \"aapl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd8c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c88a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mentions(df, ticker, social_type='comment', filter_type=1):\n",
    "    if social_type == 'comment':\n",
    "        # Dropping any potential NA's\n",
    "        df = df[df['body'].notna()]\n",
    "\n",
    "        if filter_type == 1:\n",
    "            # Filter out comments that do not mention company ticker using 'filter_data_1'\n",
    "            df_filter = df['body'].apply(filter_data_1, ticker=ticker)\n",
    "        elif filter_type == 2:\n",
    "            # Filter out comments that do not mention company ticker using 'filter_data_2'\n",
    "            df_filter = df['body'].apply(filter_data_2, ticker=ticker)\n",
    "        else:\n",
    "            raise Exception(\"Please select a valid filter_type for func [filter_mentions]. Either 1 or 2.\")\n",
    "        df = df[df_filter]\n",
    "        \n",
    "        # Skip cleaning if df is empty\n",
    "        if df.shape[0] > 0:\n",
    "            # Clean text\n",
    "            df['body'] = df['body'].apply(clean_text)  \n",
    "            \n",
    "    elif social_type == 'post':\n",
    "        # Dropping any potential NA's\n",
    "        df = df[df['selftext'].notna()]\n",
    "\n",
    "        if filter_type == 1:\n",
    "            # Filter out posts that do not mention company ticker using 'filter_data_1'\n",
    "            selftext_filter = df['selftext'].apply(filter_data_1, ticker=ticker)\n",
    "            title_filter = df['title'].apply(filter_data_1, ticker=ticker)\n",
    "        elif filter_type == 2:\n",
    "            # Filter out posts that do not mention company ticker using 'filter_data_2'\n",
    "            selftext_filter = df['selftext'].apply(filter_data_2, ticker=ticker)\n",
    "            title_filter = df['title'].apply(filter_data_2, ticker=ticker)\n",
    "        else:\n",
    "            raise Exception(\"Please select a valid filter_type for func [filter_mentions]. Either 1 or 2.\")\n",
    "        df = df[selftext_filter | title_filter]\n",
    "        \n",
    "        \n",
    "        # Skip cleaning if df is empty\n",
    "        if df.shape[0] > 0:\n",
    "            # Clean text\n",
    "            df['selftext'] = df['selftext'].apply(clean_text)\n",
    "           \n",
    "        # For posts specifically, I drop observations for\n",
    "        # - Authors postings multiple posts on 1 day (spam or duped posts)\n",
    "        df = df.drop_duplicates(subset=['author', 'date'], keep=False)\n",
    "        # - Posts with duplicate texts\n",
    "        df = df.drop_duplicates(subset=['selftext'], keep=False)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Please select a valid social_type for func [filter_mentions]. Either 'comment' or 'post'.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_df(df, save_path):\n",
    "    # Check if file already exists. Ignore headers if true\n",
    "    if os.path.isfile(save_path):\n",
    "        df.to_csv(save_path, mode='a', header=False, index=False)\n",
    "    else: \n",
    "        df.to_csv(save_path, encoding='utf-8', index=False)\n",
    "        print(f\"Creating new file at [{save_path}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d4df5",
   "metadata": {},
   "source": [
    "## Reddit method 1 <a class=\"anchor\" id=\"bullet3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9fba0",
   "metadata": {},
   "source": [
    "**Extract all comments which mention a company ticker or name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeae60fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AAPL.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AMD.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AMZN.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/ATVI.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BA.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BABA.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BAC.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/DIS.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/F.csv]nfiltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/GE.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/GME.csv]iltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/IQ.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/LULU.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/MSFT.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/MU.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/NFLX.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/NVDA.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SBUX.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SHOP.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SNAP.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SQ.csv]filtered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/TLRY.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/TSLA.csv]ltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/V.csv]nfiltered\\investing\\2018_04.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/WMT.csv]iltered\\investing\\2018_04.csv]\n",
      "Time passed: 35953.8  - Processing WMT  for [E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\\wallstreetbets\\2020_08.csv]\r"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\"\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "\n",
    "if save:\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Dropping certain columns\n",
    "            df.drop(columns=['score_hidden', 'total_awards_received', 'nest_level', 'author_fullname'], inplace=True)\n",
    "            \n",
    "            # Loop tickers\n",
    "            for ticker in ticker_list:\n",
    "                print(f\"Time passed: {str(round((time.time() - start_time), 1)).ljust(8)} - Processing {ticker.ljust(4)} for [{csv_path}]\", end='\\r')\n",
    "\n",
    "                # ----------------------- Filter_1 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(work_df, ticker, social_type='comment', filter_type=1)\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)\n",
    "\n",
    "                # ----------------------- Filter_2 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(work_df, ticker, social_type='comment', filter_type=2)\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c773b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae6caab8",
   "metadata": {},
   "source": [
    "## Reddit method 2 <a class=\"anchor\" id=\"bullet4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26e456",
   "metadata": {},
   "source": [
    "Method 2 checks what posts contain mentions of companies in the form of ticker or company name mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e90f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/AAPL.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/AAPL.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/AMD.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/AMD.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/AMZN.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/AMZN.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/ATVI.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/ATVI.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/BA.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/BA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/BABA.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/BABA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/BAC.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/BAC.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/DIS.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/DIS.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/F.csv]nfiltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/F.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/GE.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/GE.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/GME.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/GME.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/IQ.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/IQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/LULU.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/LULU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/MSFT.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/MSFT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/MU.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/MU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/NFLX.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/NFLX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/NVDA.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/NVDA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SBUX.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SBUX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SHOP.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SHOP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SNAP.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SNAP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/SQ.csv]filtered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/SQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/TLRY.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/TLRY.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/TSLA.csv]ltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/TSLA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/V.csv]nfiltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/V.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_1/WMT.csv]iltered\\filtered_df.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/posts/filtered/filter_2/WMT.csv]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\unfiltered\"\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\filtered\"\n",
    "\n",
    "if save:\n",
    "    start_time = time.time()\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Loop tickers\n",
    "            for ticker in ticker_list:\n",
    "                print(f\"Time passed: {str(round((time.time() - start_time), 1)).ljust(8)} - Processing {ticker.ljust(4)} for [{csv_path}]\", end='\\r')\n",
    "                # ----------------------- Filter_1 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(df, ticker, social_type='post', filter_type=1)\n",
    "\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)\n",
    "\n",
    "                # ----------------------- Filter_2 -----------------------\n",
    "                work_df = df.copy()\n",
    "                # Apply filter\n",
    "                filtered_df = filter_mentions(work_df, ticker, social_type='post', filter_type=2)\n",
    "\n",
    "                # Creating save_path and saving file (either new file or appending)\n",
    "                save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "                save_df(filtered_df, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c7c88",
   "metadata": {},
   "source": [
    "**Merging all comments to a single file**\n",
    "\n",
    "After finding all posts mentioning a company (besides the comments as done under method 1), it is now time to see what replies are posted to these comments. For posts these replies will be top level comments. For replies on other comments, they can be found on any level. \n",
    "\n",
    "I begin by merging all comments into a single file. This does not serve any purpose besides making it easier to loop the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93649717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "# --- Merge all comments into single file ---\n",
    "if save:\n",
    "    rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\"\n",
    "    save_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv\"\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # Create csv_path\n",
    "            csv_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read csv\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            df.drop(columns=['score_hidden', 'total_awards_received', 'nest_level', 'author_fullname'], inplace=True)\n",
    "\n",
    "            # Save new csv or append to existing csv\n",
    "            save_df(df, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3fb30",
   "metadata": {},
   "source": [
    "**Searching for replies**\n",
    "\n",
    "Method 2 also finds all comments which are replies to posts or comments containing the company name or ticker. \n",
    "\n",
    "To find out whether a comment is a reply, each comment will be checked to see if the `parent_id` matches an `id` of a comment containing a company mention. For responses to posts, the `parent_id` will be empty. For these, the `link_id` will need to match the post's id.\n",
    "\n",
    "For replies to comments:\n",
    "- `df['parent_id']` matches an `df['id']` of a comment containing a company mention.\n",
    "\n",
    "For top-level replies to posts:\n",
    "- `df['parent_id']` is empty and `df['link_id']` is equal to the post's `df['id']` which mentions a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbca6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_replies(df_chunk, comments_df, posts_df):\n",
    "    #           --- Comment filter ---\n",
    "    # Create a dataframe filter which checks if comment is a reply to other comment\n",
    "    comment_id_list = comments_df['id'].to_list()\n",
    "    comment_filter = df_chunk['parent_id'].isin(comment_id_list)\n",
    "    \n",
    "    #           --- Post filter ---\n",
    "    # Removing 't3_' from the comment 'link_id' field\n",
    "    df_chunk['link_id'] = df_chunk['link_id'].str[3:]\n",
    "    \n",
    "    # Create a dataframe filter which checks if comment is a reply to other comment\n",
    "    post_id_list = posts_df['id'].to_list()\n",
    "    post_filter = (df_chunk['parent_id'].isna() & df_chunk['link_id'].isin(post_id_list))\n",
    "    \n",
    "    return_df = df_chunk[comment_filter | post_filter]\n",
    "    return return_df\n",
    "\n",
    "# Checks replies for the chunk dataframe input\n",
    "def check_replies(df):\n",
    "    \n",
    "    ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "    comment_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "    posts_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\filtered\"\n",
    "    save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\replies\"\n",
    "    \n",
    "    for ticker in ticker_list:\n",
    "        # ----------------------- Filter_1 -----------------------\n",
    "        work_df = df.copy()\n",
    "        # Creating paths\n",
    "        comment_path = os.path.join(comment_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        posts_path = os.path.join(posts_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        \n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        posts_df = pd.read_csv(posts_path)\n",
    "        \n",
    "        # Run function\n",
    "        return_df = filter_replies(work_df, comments_df, posts_df)\n",
    "\n",
    "        # Creating save_path and saving file (either new file or appending)\n",
    "        save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_df(return_df, save_path)\n",
    "\n",
    "        # ----------------------- Filter_2 -----------------------\n",
    "        work_df = df.copy()\n",
    "        # Creating paths\n",
    "        comment_path = os.path.join(comment_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        posts_path = os.path.join(posts_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        posts_df = pd.read_csv(posts_path)\n",
    "        \n",
    "        # Run function\n",
    "        return_df = filter_replies(work_df, comments_df, posts_df)\n",
    "\n",
    "        # Creating save_path and saving file (either new file or appending)\n",
    "        save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_df(return_df, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1734d07",
   "metadata": {},
   "source": [
    "**Search for replies by chunking all_comments.csv**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8f6c5",
   "metadata": {},
   "source": [
    "To loop all comments, I loop the chunks from the all_comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d7e36f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/AAPL.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/AAPL.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/AMD.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/AMD.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/AMZN.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/AMZN.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/ATVI.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/ATVI.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/BA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/BA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/BABA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/BABA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/BAC.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/BAC.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/DIS.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/DIS.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/F.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/F.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/GE.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/GE.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/GME.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/GME.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/IQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/IQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/LULU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/LULU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/MSFT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/MSFT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/MU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/MU.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/NFLX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/NFLX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/NVDA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/NVDA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SBUX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SBUX.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SHOP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SHOP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SNAP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SNAP.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/SQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/SQ.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/TLRY.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/TLRY.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/TSLA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/TSLA.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/V.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/V.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_1/WMT.csv]\n",
      "Creating new file at [E:/Users/Christiaan/Large_Files/Thesis/reddit/replies/filter_2/WMT.csv]\n",
      "Processing chunk 21 --- time passed: 2144.2\r"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv\"\n",
    "chunksize = 10 ** 6\n",
    "counter = 1\n",
    "\n",
    "if save:\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize):\n",
    "        print(f\"Processing chunk {str(counter).ljust(2)} --- time passed: {round((time.time() - start_time), 1)}\", end='\\r')\n",
    "        check_replies(chunk)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c871618",
   "metadata": {},
   "source": [
    "**Merging comments with replies**\n",
    "\n",
    "Besides this, duplicates are also removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e55e7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "comment_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "reply_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\replies\"\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\method_2\"\n",
    "\n",
    "if save:\n",
    "    for ticker in ticker_list:\n",
    "        # ----------------------- Filter_1 -----------------------\n",
    "        comment_path = os.path.join(comment_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        reply_path = os.path.join(reply_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_path = os.path.join(save_dir, \"filter_1\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        reply_df = pd.read_csv(reply_path)\n",
    "\n",
    "        # Merge files\n",
    "        merged_df = pd.concat([comments_df, reply_df], ignore_index=True)\n",
    "\n",
    "        # Delete duplicates\n",
    "        merged_df.drop_duplicates(subset=['permalink'], inplace=True)\n",
    "\n",
    "        # Saving file\n",
    "        merged_df.to_csv(save_path, encoding='utf-8', index=False)\n",
    "\n",
    "        # ----------------------- Filter_2 -----------------------\n",
    "        comment_path = os.path.join(comment_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        reply_path = os.path.join(reply_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        save_path = os.path.join(save_dir, \"filter_2\", f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "\n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "        reply_df = pd.read_csv(reply_path)\n",
    "\n",
    "        # Merge files\n",
    "        merged_df = pd.concat([comments_df, reply_df], ignore_index=True)\n",
    "\n",
    "        # Delete duplicates\n",
    "        merged_df.drop_duplicates(subset=['permalink'], inplace=True) \n",
    "\n",
    "        # Saving file\n",
    "        merged_df.to_csv(save_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11154526",
   "metadata": {},
   "source": [
    "## Sentiment calculations <a class=\"anchor\" id=\"bullet5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d28f7",
   "metadata": {},
   "source": [
    "Having the results for all four different 'Reddit-method' - filter combinations, I can now start with sentiment calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2ed85",
   "metadata": {},
   "source": [
    "### VADER sentiment <a class=\"anchor\" id=\"sub-bullet5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb251d9",
   "metadata": {},
   "source": [
    "**Functions**\n",
    "\n",
    "These functions are partially copied from the Twitter sentiment Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60415345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f84282be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding word-sentiment pairs to the cjhutto vaderSentiment library.\n",
    "new_words = {}\n",
    "\n",
    "# Adding custom postive words\n",
    "positive_words = {\n",
    "    'buy': 2.0,\n",
    "    'buying': 2.0,\n",
    "    'bullish': 2.0,\n",
    "    'long': 1.0,\n",
    "    'call': 1.0,\n",
    "    'calls': 1.0,\n",
    "    'rocket': 3.0,        # Added for 'rocket' emoji 🚀\n",
    "    'increasing': 2.0,     # Added for 'chart increasing' emoji 📈\n",
    "    'to the moon': 2.5,\n",
    "    \"undervalued\": 2.0\n",
    "}\n",
    "# Adding custom negative words\n",
    "negative_words = {\n",
    "    'decreasing': -2.0,   # Added for 'chart increasing' emoji 📉\n",
    "    'sell': -2.0,\n",
    "    'selling': -2.0,\n",
    "    'bearish': -2.0,\n",
    "    'put': -1,\n",
    "    'puts': -1,\n",
    "    'short': -1.0,\n",
    "    'shorting': -1.5,\n",
    "    \"overvalued\": -2.0,\n",
    "    'expensive': -1.5\n",
    "}\n",
    "\n",
    "# Adding positive and negative words to new_worddictionary\n",
    "new_words.update(positive_words)\n",
    "new_words.update(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2acbe51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ---------------------------   Sentiment   ---------------------------\n",
    "# Creating SIA, which uses standard words.\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calc_sentiment_1(text, sent_type):\n",
    "    result = SIA.polarity_scores(text)\n",
    "    return result[sent_type]\n",
    "\n",
    "# Creating SIA2 to add custom words.\n",
    "SIA2 = SentimentIntensityAnalyzer()\n",
    "SIA2.lexicon.update(new_words)\n",
    "\n",
    "def calc_sentiment_2(text, sent_type):\n",
    "    result = SIA2.polarity_scores(text)\n",
    "    return result[sent_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "da9699b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, ticker):\n",
    "    start_time = time.time()\n",
    "    # Prepping data by setting date column\n",
    "    df['created_at'] = pd.to_datetime(df['utc_datetime_str'])\n",
    "    df['date'] = df['created_at'].dt.date\n",
    "    \n",
    "    #   ---------------------------   Sentiment   ---------------------------\n",
    "    # Calculate sentiment scores\n",
    "    df[f'compound_sent_1'] = df['body'].astype(str).apply(calc_sentiment_1, sent_type='compound')\n",
    "    df[f'compound_sent_2'] = df['body'].astype(str).apply(calc_sentiment_2, sent_type='compound')\n",
    "    \n",
    "    print(f\"[{ticker}] Done calculating sentiment after --- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    \"\"\"Converting this to pos, neg or neu sentiment\n",
    "    - positive sentiment: compound score >= 0.05\n",
    "    - neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    - negative sentiment: compound score <= -0.05\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if sentiment corresponds to pos, neg or neu sentiment for compound_sent_1\n",
    "    df['s1_pos'] = np.where(df['compound_sent_1'] >= 0.05, 1, 0)\n",
    "    df['s1_neg'] = np.where(df['compound_sent_1'] <= -0.05, 1, 0)\n",
    "\n",
    "    # Check if sentiment corresponds to pos, neg or neu sentiment for compound_sent_2\n",
    "    df['s2_pos'] = np.where(df['compound_sent_2'] >= 0.05, 1, 0)\n",
    "    df['s2_neg'] = np.where(df['compound_sent_2'] <= -0.05, 1, 0)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c47f34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_posts(df):\n",
    "    # Create results_df with [sentiment_1]\n",
    "    results_df = df[['date', 's1_pos', 's1_neg']].groupby('date', as_index=False).sum().rename(columns={\"s1_pos\": \"[s1]pos\", \"s1_neg\": \"[s1]neg\"})\n",
    "    results_df['[s1]total'] = results_df['[s1]pos'] + results_df['[s1]neg']\n",
    "\n",
    "    # Merge [sentiment_2]\n",
    "    to_merge_df = df[['date', 's2_pos', 's2_neg']].groupby('date', as_index=False).sum().rename(columns={\"s2_pos\": \"[s2]pos\", \"s2_neg\": \"[s2]neg\"})\n",
    "    to_merge_df['[s2]total'] = to_merge_df['[s2]pos'] + to_merge_df['[s2]neg']\n",
    "    results_df = results_df.merge(to_merge_df, how='left', left_on='date', right_on='date')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eedc96",
   "metadata": {},
   "source": [
    "**Different types of VADER sentiment measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce24d7",
   "metadata": {},
   "source": [
    "Although different data filters and sentiment measuring methods are mentioned, this does not solve the problem of how sentiment needs to be measured. Do I take the total amount of positive posts as a proxy of sentiment? Or do I use the ratio of positive and negative posts. Or do I subtract the negative posts from the positive posts and divide that figure by the total amount of posts?\n",
    "\n",
    "It is clear that there is no clear winner here. I propose that sentiment should be measure in two parts. The first part should contain the `relative volume` of the social media posts, as large volumes are more likely to be noticable. Part 1 will thus act as a way to strengen or weaken the total sentiment score of the day, by comparing the volume of that day with the average volume of the last 7 days. The second part of the measure should contain the actual `sentiment`. Is it positive or negative? It should also capture the severity of the sentiment. \n",
    "\n",
    "\n",
    "**Part 1**\n",
    "The first part is the easier one of the two parts to measure. With the sentiment tools all returning either a positive, negative or neutral label, I decide that I will solely be focussing on the positive and negative posts. This means that these are also the posts that will be taken into considerations when looking at volume. The volume will be measured by counting the total of positive and negative posts for a given day. This daily total will then be divided by the average total posts of the last 7 days.\n",
    "\n",
    "- $\\text{Relative volume}_{t0} = \\frac{\\text{Total positive posts}_{t0}+\\text{Total negative posts}_{t0}}{Rolling mean 30(\\text{Total positive posts}_{t0}+\\text{Total negative posts}_{t0})}$\n",
    "\n",
    "**Part 2**\n",
    "The second part is harded, as it is unclear what the best way to measure sentiment is. To tackle this problem, I will calculate the sentiment in different ways.\n",
    "- Method 1: positive / negative\n",
    "- <strike>Method 2: (positive - negative) / (positive + negative)</strike>\n",
    "- Method 3: Daily mean of compound sentiment score, counting only posts categorised as positive or negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fbd8ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_measures(return_df):\n",
    "    # Get results dataframe\n",
    "    sentiment_measures = count_posts(return_df)\n",
    "\n",
    "    # Only keep sentiment 2 results\n",
    "    sentiment_measures = sentiment_measures.loc[:, sentiment_measures.columns.str.contains('s1|s2|date')]\n",
    "    \n",
    "    # Method 1 - ratio\n",
    "    sentiment_measures['[s1]method_1'] = sentiment_measures['[s1]pos'] / sentiment_measures['[s1]total']\n",
    "    sentiment_measures['[s2]method_1'] = sentiment_measures['[s2]pos'] / sentiment_measures['[s2]total']\n",
    "\n",
    "    # Method 2 - discontinued as it is the same as method 1\n",
    "    #     sentiment_measures['[f1s2]method_2'] = (sentiment_measures['[f1s2]pos'] - sentiment_measures['[f1s2]neg']) / sentiment_measures['[f1s2]total']\n",
    "    #     sentiment_measures['[f2s2]method_2'] = (sentiment_measures['[f2s2]pos'] - sentiment_measures['[f2s2]neg']) / sentiment_measures['[f2s2]total']\n",
    "\n",
    "    # Method 3\n",
    "    to_merge = return_df[((return_df['s1_pos'] == 1) | (return_df['s1_neg'] == 1))][['date', 'compound_sent_1']].groupby('date').mean().rename(columns={'compound_sent_1': '[s1]method_3'})\n",
    "    sentiment_measures = sentiment_measures.merge(to_merge, how='left', left_on='date', right_on='date')\n",
    "    to_merge = return_df[((return_df['s2_pos'] == 1) | (return_df['s2_neg'] == 1))][['date', 'compound_sent_2']].groupby('date').mean().rename(columns={'compound_sent_2': '[s2]method_3'})\n",
    "\n",
    "    sentiment_measures = sentiment_measures.merge(to_merge, how='left', left_on='date', right_on='date')\n",
    "    return sentiment_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383edc92",
   "metadata": {},
   "source": [
    "**Actual sentiment calculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c0f5764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AAPL] Done calculating sentiment after --- 204.51891446113586 seconds ---\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/AAPL.csv\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/AAPL.csv\n",
      "\n",
      "[AAPL] Done calculating sentiment after --- 59.460816621780396 seconds ---\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AAPL.csv\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/AAPL.csv\n",
      "\n",
      "[AAPL] Done calculating sentiment after --- 263.22150135040283 seconds ---\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/AAPL.csv\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/AAPL.csv\n",
      "\n",
      "[AAPL] Done calculating sentiment after --- 82.80350613594055 seconds ---\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/AAPL.csv\n",
      "[AAPL] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/AAPL.csv\n",
      "\n",
      "[AMD] Done calculating sentiment after --- 94.48554968833923 seconds ---\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/AMD.csv\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/AMD.csv\n",
      "\n",
      "[AMD] Done calculating sentiment after --- 44.73735022544861 seconds ---\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AMD.csv\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/AMD.csv\n",
      "\n",
      "[AMD] Done calculating sentiment after --- 138.7493975162506 seconds ---\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/AMD.csv\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/AMD.csv\n",
      "\n",
      "[AMD] Done calculating sentiment after --- 74.78809857368469 seconds ---\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/AMD.csv\n",
      "[AMD] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/AMD.csv\n",
      "\n",
      "[AMZN] Done calculating sentiment after --- 171.52032804489136 seconds ---\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/AMZN.csv\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/AMZN.csv\n",
      "\n",
      "[AMZN] Done calculating sentiment after --- 47.07003474235535 seconds ---\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/AMZN.csv\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/AMZN.csv\n",
      "\n",
      "[AMZN] Done calculating sentiment after --- 216.89763069152832 seconds ---\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/AMZN.csv\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/AMZN.csv\n",
      "\n",
      "[AMZN] Done calculating sentiment after --- 72.58526921272278 seconds ---\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/AMZN.csv\n",
      "[AMZN] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/AMZN.csv\n",
      "\n",
      "[ATVI] Done calculating sentiment after --- 1.9939618110656738 seconds ---\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/ATVI.csv\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/ATVI.csv\n",
      "\n",
      "[ATVI] Done calculating sentiment after --- 0.7732853889465332 seconds ---\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/ATVI.csv\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/ATVI.csv\n",
      "\n",
      "[ATVI] Done calculating sentiment after --- 5.630345344543457 seconds ---\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/ATVI.csv\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/ATVI.csv\n",
      "\n",
      "[ATVI] Done calculating sentiment after --- 1.6638691425323486 seconds ---\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/ATVI.csv\n",
      "[ATVI] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/ATVI.csv\n",
      "\n",
      "[BA] Done calculating sentiment after --- 36.98866105079651 seconds ---\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/BA.csv\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/BA.csv\n",
      "\n",
      "[BA] Done calculating sentiment after --- 19.15673279762268 seconds ---\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BA.csv\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/BA.csv\n",
      "\n",
      "[BA] Done calculating sentiment after --- 56.517839670181274 seconds ---\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/BA.csv\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/BA.csv\n",
      "\n",
      "[BA] Done calculating sentiment after --- 29.057360410690308 seconds ---\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/BA.csv\n",
      "[BA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/BA.csv\n",
      "\n",
      "[BABA] Done calculating sentiment after --- 10.455277442932129 seconds ---\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/BABA.csv\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/BABA.csv\n",
      "\n",
      "[BABA] Done calculating sentiment after --- 3.241753578186035 seconds ---\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BABA.csv\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/BABA.csv\n",
      "\n",
      "[BABA] Done calculating sentiment after --- 17.469873189926147 seconds ---\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/BABA.csv\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/BABA.csv\n",
      "\n",
      "[BABA] Done calculating sentiment after --- 5.204100608825684 seconds ---\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/BABA.csv\n",
      "[BABA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/BABA.csv\n",
      "\n",
      "[BAC] Done calculating sentiment after --- 7.520293712615967 seconds ---\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/BAC.csv\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/BAC.csv\n",
      "\n",
      "[BAC] Done calculating sentiment after --- 2.3479349613189697 seconds ---\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/BAC.csv\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/BAC.csv\n",
      "\n",
      "[BAC] Done calculating sentiment after --- 11.7234046459198 seconds ---\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/BAC.csv\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/BAC.csv\n",
      "\n",
      "[BAC] Done calculating sentiment after --- 3.6023619174957275 seconds ---\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/BAC.csv\n",
      "[BAC] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/BAC.csv\n",
      "\n",
      "[DIS] Done calculating sentiment after --- 58.916221141815186 seconds ---\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/DIS.csv\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/DIS.csv\n",
      "\n",
      "[DIS] Done calculating sentiment after --- 21.49347758293152 seconds ---\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/DIS.csv\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/DIS.csv\n",
      "\n",
      "[DIS] Done calculating sentiment after --- 84.46442031860352 seconds ---\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/DIS.csv\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/DIS.csv\n",
      "\n",
      "[DIS] Done calculating sentiment after --- 35.051100969314575 seconds ---\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/DIS.csv\n",
      "[DIS] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/DIS.csv\n",
      "\n",
      "[F] Done calculating sentiment after --- 160.31656289100647 seconds ---\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/F.csv\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/F.csv\n",
      "\n",
      "[F] Done calculating sentiment after --- 12.946342945098877 seconds ---\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/F.csv\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/F.csv\n",
      "\n",
      "[F] Done calculating sentiment after --- 226.34678053855896 seconds ---\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/F.csv\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/F.csv\n",
      "\n",
      "[F] Done calculating sentiment after --- 22.896841287612915 seconds ---\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/F.csv\n",
      "[F] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/F.csv\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GE] Done calculating sentiment after --- 3.4391849040985107 seconds ---\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/GE.csv\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/GE.csv\n",
      "\n",
      "[GE] Done calculating sentiment after --- 0.3592112064361572 seconds ---\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/GE.csv\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/GE.csv\n",
      "\n",
      "[GE] Done calculating sentiment after --- 5.959677457809448 seconds ---\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/GE.csv\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/GE.csv\n",
      "\n",
      "[GE] Done calculating sentiment after --- 0.8680615425109863 seconds ---\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/GE.csv\n",
      "[GE] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/GE.csv\n",
      "\n",
      "[GME] Done calculating sentiment after --- 7.820158004760742 seconds ---\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/GME.csv\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/GME.csv\n",
      "\n",
      "[GME] Done calculating sentiment after --- 2.258622646331787 seconds ---\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/GME.csv\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/GME.csv\n",
      "\n",
      "[GME] Done calculating sentiment after --- 10.960918426513672 seconds ---\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/GME.csv\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/GME.csv\n",
      "\n",
      "[GME] Done calculating sentiment after --- 3.8311266899108887 seconds ---\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/GME.csv\n",
      "[GME] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/GME.csv\n",
      "\n",
      "[IQ] Done calculating sentiment after --- 1.8279097080230713 seconds ---\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/IQ.csv\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/IQ.csv\n",
      "\n",
      "[IQ] Done calculating sentiment after --- 0.2754678726196289 seconds ---\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/IQ.csv\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/IQ.csv\n",
      "\n",
      "[IQ] Done calculating sentiment after --- 4.784825325012207 seconds ---\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/IQ.csv\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/IQ.csv\n",
      "\n",
      "[IQ] Done calculating sentiment after --- 0.7477161884307861 seconds ---\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/IQ.csv\n",
      "[IQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/IQ.csv\n",
      "\n",
      "[LULU] Done calculating sentiment after --- 1.8748774528503418 seconds ---\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/LULU.csv\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/LULU.csv\n",
      "\n",
      "[LULU] Done calculating sentiment after --- 0.4838902950286865 seconds ---\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/LULU.csv\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/LULU.csv\n",
      "\n",
      "[LULU] Done calculating sentiment after --- 3.3652024269104004 seconds ---\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/LULU.csv\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/LULU.csv\n",
      "\n",
      "[LULU] Done calculating sentiment after --- 1.0731899738311768 seconds ---\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/LULU.csv\n",
      "[LULU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/LULU.csv\n",
      "\n",
      "[MSFT] Done calculating sentiment after --- 67.78290128707886 seconds ---\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/MSFT.csv\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/MSFT.csv\n",
      "\n",
      "[MSFT] Done calculating sentiment after --- 16.830890893936157 seconds ---\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/MSFT.csv\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/MSFT.csv\n",
      "\n",
      "[MSFT] Done calculating sentiment after --- 101.25723457336426 seconds ---\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/MSFT.csv\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/MSFT.csv\n",
      "\n",
      "[MSFT] Done calculating sentiment after --- 27.547935724258423 seconds ---\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/MSFT.csv\n",
      "[MSFT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/MSFT.csv\n",
      "\n",
      "[MU] Done calculating sentiment after --- 3.4354958534240723 seconds ---\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/MU.csv\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/MU.csv\n",
      "\n",
      "[MU] Done calculating sentiment after --- 1.722496747970581 seconds ---\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/MU.csv\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/MU.csv\n",
      "\n",
      "[MU] Done calculating sentiment after --- 6.3199543952941895 seconds ---\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/MU.csv\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/MU.csv\n",
      "\n",
      "[MU] Done calculating sentiment after --- 3.50093674659729 seconds ---\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/MU.csv\n",
      "[MU] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/MU.csv\n",
      "\n",
      "[NFLX] Done calculating sentiment after --- 59.02890062332153 seconds ---\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/NFLX.csv\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/NFLX.csv\n",
      "\n",
      "[NFLX] Done calculating sentiment after --- 16.650238275527954 seconds ---\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/NFLX.csv\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/NFLX.csv\n",
      "\n",
      "[NFLX] Done calculating sentiment after --- 81.52620100975037 seconds ---\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/NFLX.csv\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/NFLX.csv\n",
      "\n",
      "[NFLX] Done calculating sentiment after --- 29.30825161933899 seconds ---\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/NFLX.csv\n",
      "[NFLX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/NFLX.csv\n",
      "\n",
      "[NVDA] Done calculating sentiment after --- 20.734434127807617 seconds ---\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/NVDA.csv\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/NVDA.csv\n",
      "\n",
      "[NVDA] Done calculating sentiment after --- 3.2232701778411865 seconds ---\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/NVDA.csv\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/NVDA.csv\n",
      "\n",
      "[NVDA] Done calculating sentiment after --- 31.386186838150024 seconds ---\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/NVDA.csv\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/NVDA.csv\n",
      "\n",
      "[NVDA] Done calculating sentiment after --- 5.922730445861816 seconds ---\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/NVDA.csv\n",
      "[NVDA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/NVDA.csv\n",
      "\n",
      "[SBUX] Done calculating sentiment after --- 14.656370162963867 seconds ---\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/SBUX.csv\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/SBUX.csv\n",
      "\n",
      "[SBUX] Done calculating sentiment after --- 5.562410593032837 seconds ---\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SBUX.csv\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/SBUX.csv\n",
      "\n",
      "[SBUX] Done calculating sentiment after --- 21.45138955116272 seconds ---\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/SBUX.csv\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/SBUX.csv\n",
      "\n",
      "[SBUX] Done calculating sentiment after --- 8.500197172164917 seconds ---\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/SBUX.csv\n",
      "[SBUX] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/SBUX.csv\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHOP] Done calculating sentiment after --- 12.072792053222656 seconds ---\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/SHOP.csv\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/SHOP.csv\n",
      "\n",
      "[SHOP] Done calculating sentiment after --- 3.521402359008789 seconds ---\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SHOP.csv\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/SHOP.csv\n",
      "\n",
      "[SHOP] Done calculating sentiment after --- 19.06176519393921 seconds ---\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/SHOP.csv\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/SHOP.csv\n",
      "\n",
      "[SHOP] Done calculating sentiment after --- 5.842756986618042 seconds ---\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/SHOP.csv\n",
      "[SHOP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/SHOP.csv\n",
      "\n",
      "[SNAP] Done calculating sentiment after --- 8.475839614868164 seconds ---\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/SNAP.csv\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/SNAP.csv\n",
      "\n",
      "[SNAP] Done calculating sentiment after --- 3.409670114517212 seconds ---\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SNAP.csv\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/SNAP.csv\n",
      "\n",
      "[SNAP] Done calculating sentiment after --- 17.373990297317505 seconds ---\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/SNAP.csv\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/SNAP.csv\n",
      "\n",
      "[SNAP] Done calculating sentiment after --- 8.350694417953491 seconds ---\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/SNAP.csv\n",
      "[SNAP] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/SNAP.csv\n",
      "\n",
      "[SQ] Done calculating sentiment after --- 61.25904369354248 seconds ---\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/SQ.csv\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/SQ.csv\n",
      "\n",
      "[SQ] Done calculating sentiment after --- 2.5943737030029297 seconds ---\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/SQ.csv\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/SQ.csv\n",
      "\n",
      "[SQ] Done calculating sentiment after --- 82.4875373840332 seconds ---\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/SQ.csv\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/SQ.csv\n",
      "\n",
      "[SQ] Done calculating sentiment after --- 4.41597580909729 seconds ---\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/SQ.csv\n",
      "[SQ] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/SQ.csv\n",
      "\n",
      "[TLRY] Done calculating sentiment after --- 3.6472294330596924 seconds ---\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/TLRY.csv\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/TLRY.csv\n",
      "\n",
      "[TLRY] Done calculating sentiment after --- 1.8667397499084473 seconds ---\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/TLRY.csv\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/TLRY.csv\n",
      "\n",
      "[TLRY] Done calculating sentiment after --- 6.59924578666687 seconds ---\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/TLRY.csv\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/TLRY.csv\n",
      "\n",
      "[TLRY] Done calculating sentiment after --- 3.4494831562042236 seconds ---\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/TLRY.csv\n",
      "[TLRY] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/TLRY.csv\n",
      "\n",
      "[TSLA] Done calculating sentiment after --- 185.4114227294922 seconds ---\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/TSLA.csv\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/TSLA.csv\n",
      "\n",
      "[TSLA] Done calculating sentiment after --- 96.09692621231079 seconds ---\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/TSLA.csv\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/TSLA.csv\n",
      "\n",
      "[TSLA] Done calculating sentiment after --- 252.19317841529846 seconds ---\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/TSLA.csv\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/TSLA.csv\n",
      "\n",
      "[TSLA] Done calculating sentiment after --- 144.71493816375732 seconds ---\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/TSLA.csv\n",
      "[TSLA] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/TSLA.csv\n",
      "\n",
      "[V] Done calculating sentiment after --- 32.76523017883301 seconds ---\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/V.csv\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/V.csv\n",
      "\n",
      "[V] Done calculating sentiment after --- 7.217480897903442 seconds ---\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/V.csv\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/V.csv\n",
      "\n",
      "[V] Done calculating sentiment after --- 48.80319619178772 seconds ---\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/V.csv\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/V.csv\n",
      "\n",
      "[V] Done calculating sentiment after --- 11.45236849784851 seconds ---\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/V.csv\n",
      "[V] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/V.csv\n",
      "\n",
      "[WMT] Done calculating sentiment after --- 33.48162508010864 seconds ---\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_1/WMT.csv\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f1/WMT.csv\n",
      "\n",
      "[WMT] Done calculating sentiment after --- 8.713562250137329 seconds ---\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/filter_2/WMT.csv\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m1f2/WMT.csv\n",
      "\n",
      "[WMT] Done calculating sentiment after --- 45.433185338974 seconds ---\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_1/WMT.csv\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f1/WMT.csv\n",
      "\n",
      "[WMT] Done calculating sentiment after --- 13.789832353591919 seconds ---\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/method_2/filter_2/WMT.csv\n",
      "[WMT] E:/Users/Christiaan/Large_Files/Thesis/reddit/sentiment/m2f2/WMT.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\sentiment\\VADER\"\n",
    "methods = ['m1f1', 'm2f1', 'm1f2', 'm2f2']\n",
    "method_1_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "method_2_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\method_2\"\n",
    "\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    def loop_func(comment_dir, filter_method=\"filter_1\", method_name=\"m1f1\"):\n",
    "        # ----------------------- m1f1/m1f2/m2f1/m2f2 -----------------------\n",
    "        comment_path = os.path.join(comment_dir, filter_method, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        \n",
    "        # Reading csv's to df\n",
    "        comments_df = pd.read_csv(comment_path)\n",
    "\n",
    "        # Filter and clean data. Also perform VADER sentiment scoring.\n",
    "        return_df = clean_data(comments_df, ticker)\n",
    "\n",
    "        # Calculate sentiment scores for each method\n",
    "        sentiment_measures = calc_sent_measures(return_df)\n",
    "\n",
    "        # Creating save path and saving file\n",
    "        save_path = os.path.join(save_dir, method_name, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "        sentiment_measures.to_csv(save_path, encoding='utf-8', index=False)\n",
    "        \n",
    "        print(f\"[{ticker}]\", comment_path)\n",
    "        print(f\"[{ticker}]\", save_path)\n",
    "        print()\n",
    "    \n",
    "    # For each of the 4 method-filter combinations, run the the functions\n",
    "    loop_func(comment_dir=method_1_dir, filter_method=\"filter_1\",  method_name=\"m1f1\")\n",
    "    loop_func(comment_dir=method_1_dir, filter_method=\"filter_2\",  method_name=\"m1f2\")\n",
    "    loop_func(comment_dir=method_2_dir, filter_method=\"filter_1\",  method_name=\"m2f1\")\n",
    "    loop_func(comment_dir=method_2_dir, filter_method=\"filter_2\",  method_name=\"m2f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66221e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e4905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fafe65b0",
   "metadata": {},
   "source": [
    "### finBERT sentiment <a class=\"anchor\" id=\"sub-bullet5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28489294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc329dbb",
   "metadata": {},
   "source": [
    "## ToDo <a class=\"anchor\" id=\"ToDo\"></a>\n",
    "\n",
    "[Go back up](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f29333",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'author': 'str',\n",
    "         'author_fullname': 'str',\n",
    "         'created_utc': 'int64',\n",
    "         'permalink': 'str',\n",
    "         'score': 'int64',\n",
    "         'body': 'str',\n",
    "         'is_submitter': 'bool',\n",
    "         'id': 'str',\n",
    "         'link_id': 'str',\n",
    "         'parent_id': 'str',\n",
    "         'nest_level': 'float64',\n",
    "         'subreddit': 'str',\n",
    "         'subreddit_id': 'str'}\n",
    "\n",
    "csv_path = r\"E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/AAPL.csv\"\n",
    "df = pd.read_csv(csv_path, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11efa33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000     True\n",
       "2000001    False\n",
       "2000002    False\n",
       "2000003     True\n",
       "2000004     True\n",
       "           ...  \n",
       "2999995    False\n",
       "2999996     True\n",
       "2999997    False\n",
       "2999998     True\n",
       "2999999    False\n",
       "Name: parent_id, Length: 1000000, dtype: bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk['parent_id'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9389718",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\replies\\filter_1\\AMD.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\\filter_1\\AMD.csv\"\n",
    "df2 = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "557154a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267018, 12)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.concat([df, df2], ignore_index=True)\n",
    "# df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "974e11a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267018, 12)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.drop_duplicates(subset=['author', 'created_utc']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a1cb8111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267066, 12)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.drop_duplicates(subset=['author', 'id']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b4edc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267066, 12)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.drop_duplicates(subset=['permalink']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69fd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9dc81a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>nest_level</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moldyjellybean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1527772177</td>\n",
       "      <td>2018-05-31 13:09:37</td>\n",
       "      <td>/r/investing/comments/8nhv7o/daily_advice_thre...</td>\n",
       "      <td>2</td>\n",
       "      <td>Interactive Brokers, Fidelity, Schwab. Which p...</td>\n",
       "      <td>False</td>\n",
       "      <td>dzvm8me</td>\n",
       "      <td>t3_8nhv7o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Archdukeprinceking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1527755558</td>\n",
       "      <td>2018-05-31 08:32:38</td>\n",
       "      <td>/r/investing/comments/8nc4vr/warren_buffett_re...</td>\n",
       "      <td>4</td>\n",
       "      <td>Yikes most all of their revenue is from consul...</td>\n",
       "      <td>False</td>\n",
       "      <td>dzvck31</td>\n",
       "      <td>t3_8nc4vr</td>\n",
       "      <td>dzv28hu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>andreabrodycloud</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1527752907</td>\n",
       "      <td>2018-05-31 07:48:27</td>\n",
       "      <td>/r/investing/comments/8nc4vr/warren_buffett_re...</td>\n",
       "      <td>5</td>\n",
       "      <td>Intel is working on 10nm for 2019 while AMD is...</td>\n",
       "      <td>False</td>\n",
       "      <td>dzvbb5b</td>\n",
       "      <td>t3_8nc4vr</td>\n",
       "      <td>dzv7ki7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hqtitan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1527749100</td>\n",
       "      <td>2018-05-31 06:45:00</td>\n",
       "      <td>/r/investing/comments/8nc4vr/warren_buffett_re...</td>\n",
       "      <td>7</td>\n",
       "      <td>IBM is mainly in high performance computing th...</td>\n",
       "      <td>False</td>\n",
       "      <td>dzv9fg3</td>\n",
       "      <td>t3_8nc4vr</td>\n",
       "      <td>dzv28hu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hakkzpets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1527745756</td>\n",
       "      <td>2018-05-31 05:49:16</td>\n",
       "      <td>/r/investing/comments/8nc4vr/warren_buffett_re...</td>\n",
       "      <td>15</td>\n",
       "      <td>&amp;gt; Do you think Intel and AMD are the ones a...</td>\n",
       "      <td>False</td>\n",
       "      <td>dzv7ki7</td>\n",
       "      <td>t3_8nc4vr</td>\n",
       "      <td>dzv28hu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135158</th>\n",
       "      <td>MoonRei_Razing</td>\n",
       "      <td>t2_2511dxfd</td>\n",
       "      <td>1596240630</td>\n",
       "      <td>2020-08-01 00:10:30</td>\n",
       "      <td>/r/wallstreetbets/comments/i1czqd/how_screwed_...</td>\n",
       "      <td>1</td>\n",
       "      <td>Being in tech, and making the decisions to pro...</td>\n",
       "      <td>False</td>\n",
       "      <td>fzxobcb</td>\n",
       "      <td>t3_i1czqd</td>\n",
       "      <td>fzwtvxk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135159</th>\n",
       "      <td>TrapHouseLessons</td>\n",
       "      <td>t2_1evejy95</td>\n",
       "      <td>1596240393</td>\n",
       "      <td>2020-08-01 00:06:33</td>\n",
       "      <td>/r/wallstreetbets/comments/i1ejxp/weekend_disc...</td>\n",
       "      <td>2</td>\n",
       "      <td>If you are a long term investor with a low ris...</td>\n",
       "      <td>False</td>\n",
       "      <td>fzxnuja</td>\n",
       "      <td>t3_i1ejxp</td>\n",
       "      <td>fzxk3kc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135160</th>\n",
       "      <td>Frostfright</td>\n",
       "      <td>t2_5tgc5n5</td>\n",
       "      <td>1596240363</td>\n",
       "      <td>2020-08-01 00:06:03</td>\n",
       "      <td>/r/wallstreetbets/comments/i1fcfi/which_is_the...</td>\n",
       "      <td>7</td>\n",
       "      <td>AMD has higher meme potential, but TSM is a gu...</td>\n",
       "      <td>False</td>\n",
       "      <td>fzxnsd1</td>\n",
       "      <td>t3_i1fcfi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135161</th>\n",
       "      <td>Coffeepillow</td>\n",
       "      <td>t2_4h49c</td>\n",
       "      <td>1596240140</td>\n",
       "      <td>2020-08-01 00:02:20</td>\n",
       "      <td>/r/wallstreetbets/comments/i1ejxp/weekend_disc...</td>\n",
       "      <td>1</td>\n",
       "      <td>Down $2500, feeling pretty bad. I could have b...</td>\n",
       "      <td>False</td>\n",
       "      <td>fzxnbxh</td>\n",
       "      <td>t3_i1ejxp</td>\n",
       "      <td>fzxkm2n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135162</th>\n",
       "      <td>KorOguy</td>\n",
       "      <td>t2_3zk9nshv</td>\n",
       "      <td>1596240068</td>\n",
       "      <td>2020-08-01 00:01:08</td>\n",
       "      <td>/r/wallstreetbets/comments/i1fcfi/which_is_the...</td>\n",
       "      <td>2</td>\n",
       "      <td>All though the consoles are low margin, AMD st...</td>\n",
       "      <td>False</td>\n",
       "      <td>fzxn6qb</td>\n",
       "      <td>t3_i1fcfi</td>\n",
       "      <td>fzxdnvy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135163 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author author_fullname  created_utc     utc_datetime_str  \\\n",
       "0           moldyjellybean             NaN   1527772177  2018-05-31 13:09:37   \n",
       "1       Archdukeprinceking             NaN   1527755558  2018-05-31 08:32:38   \n",
       "2         andreabrodycloud             NaN   1527752907  2018-05-31 07:48:27   \n",
       "3                  hqtitan             NaN   1527749100  2018-05-31 06:45:00   \n",
       "4                hakkzpets             NaN   1527745756  2018-05-31 05:49:16   \n",
       "...                    ...             ...          ...                  ...   \n",
       "135158      MoonRei_Razing     t2_2511dxfd   1596240630  2020-08-01 00:10:30   \n",
       "135159    TrapHouseLessons     t2_1evejy95   1596240393  2020-08-01 00:06:33   \n",
       "135160         Frostfright      t2_5tgc5n5   1596240363  2020-08-01 00:06:03   \n",
       "135161        Coffeepillow        t2_4h49c   1596240140  2020-08-01 00:02:20   \n",
       "135162             KorOguy     t2_3zk9nshv   1596240068  2020-08-01 00:01:08   \n",
       "\n",
       "                                                permalink  score  \\\n",
       "0       /r/investing/comments/8nhv7o/daily_advice_thre...      2   \n",
       "1       /r/investing/comments/8nc4vr/warren_buffett_re...      4   \n",
       "2       /r/investing/comments/8nc4vr/warren_buffett_re...      5   \n",
       "3       /r/investing/comments/8nc4vr/warren_buffett_re...      7   \n",
       "4       /r/investing/comments/8nc4vr/warren_buffett_re...     15   \n",
       "...                                                   ...    ...   \n",
       "135158  /r/wallstreetbets/comments/i1czqd/how_screwed_...      1   \n",
       "135159  /r/wallstreetbets/comments/i1ejxp/weekend_disc...      2   \n",
       "135160  /r/wallstreetbets/comments/i1fcfi/which_is_the...      7   \n",
       "135161  /r/wallstreetbets/comments/i1ejxp/weekend_disc...      1   \n",
       "135162  /r/wallstreetbets/comments/i1fcfi/which_is_the...      2   \n",
       "\n",
       "                                                     body  is_submitter  \\\n",
       "0       Interactive Brokers, Fidelity, Schwab. Which p...         False   \n",
       "1       Yikes most all of their revenue is from consul...         False   \n",
       "2       Intel is working on 10nm for 2019 while AMD is...         False   \n",
       "3       IBM is mainly in high performance computing th...         False   \n",
       "4       &gt; Do you think Intel and AMD are the ones a...         False   \n",
       "...                                                   ...           ...   \n",
       "135158  Being in tech, and making the decisions to pro...         False   \n",
       "135159  If you are a long term investor with a low ris...         False   \n",
       "135160  AMD has higher meme potential, but TSM is a gu...         False   \n",
       "135161  Down $2500, feeling pretty bad. I could have b...         False   \n",
       "135162  All though the consoles are low margin, AMD st...         False   \n",
       "\n",
       "             id    link_id parent_id  nest_level       subreddit subreddit_id  \n",
       "0       dzvm8me  t3_8nhv7o       NaN         1.0       investing     t5_2qhhq  \n",
       "1       dzvck31  t3_8nc4vr   dzv28hu         NaN       investing     t5_2qhhq  \n",
       "2       dzvbb5b  t3_8nc4vr   dzv7ki7         NaN       investing     t5_2qhhq  \n",
       "3       dzv9fg3  t3_8nc4vr   dzv28hu         NaN       investing     t5_2qhhq  \n",
       "4       dzv7ki7  t3_8nc4vr   dzv28hu         NaN       investing     t5_2qhhq  \n",
       "...         ...        ...       ...         ...             ...          ...  \n",
       "135158  fzxobcb  t3_i1czqd   fzwtvxk         NaN  wallstreetbets     t5_2th52  \n",
       "135159  fzxnuja  t3_i1ejxp   fzxk3kc         NaN  wallstreetbets     t5_2th52  \n",
       "135160  fzxnsd1  t3_i1fcfi       NaN         1.0  wallstreetbets     t5_2th52  \n",
       "135161  fzxnbxh  t3_i1ejxp   fzxkm2n         NaN  wallstreetbets     t5_2th52  \n",
       "135162  fzxn6qb  t3_i1fcfi   fzxdnvy         NaN  wallstreetbets     t5_2th52  \n",
       "\n",
       "[135163 rows x 14 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2\n",
    "# df2.drop(columns=['nest_level', 'author_fullname'])\n",
    "# df2.drop(columns=['nest_level', 'author_fullname'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae90ff67",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['nest_level', 'author_fullname'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14260\\603703716.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nest_level'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'author_fullname'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4955\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4956\u001b[0m         \"\"\"\n\u001b[1;32m-> 4957\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4958\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4959\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4267\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4312\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6659\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6660\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6661\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6662\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6663\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['nest_level', 'author_fullname'] not found in axis\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "596189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\\AAPL.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6b9ffc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         8nc4vr\n",
       "1         8nj0th\n",
       "2         8nj0th\n",
       "3         8nds9s\n",
       "4         8n80pn\n",
       "           ...  \n",
       "124552    i1ejxp\n",
       "124553    i1ejxp\n",
       "124554    i15376\n",
       "124555    i1ejxp\n",
       "124556    i1ejxp\n",
       "Name: link_id, Length: 124557, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_id'].str[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "688eafb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000    t3_hfwyw9\n",
       "2000001    t3_hfww9a\n",
       "2000002    t3_hfudwo\n",
       "2000003    t3_hfqy3p\n",
       "2000004    t3_hfwyw9\n",
       "             ...    \n",
       "2999995    t3_b1lden\n",
       "2999996    t3_b1nenl\n",
       "2999997    t3_b1grpq\n",
       "2999998    t3_b1grpq\n",
       "2999999    t3_b1ab6k\n",
       "Name: link_id, Length: 1000000, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk['link_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "425a0dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000     True\n",
       "2000001    False\n",
       "2000002    False\n",
       "2000003     True\n",
       "2000004     True\n",
       "           ...  \n",
       "2999995    False\n",
       "2999996     True\n",
       "2999997    False\n",
       "2999998     True\n",
       "2999999    False\n",
       "Length: 1000000, dtype: bool"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(chunk['parent_id'].isna() | chunk['link_id'].isin(['t3_hfqy3p']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af73b1ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000    False\n",
       "2000001     True\n",
       "2000002     True\n",
       "2000003    False\n",
       "2000004    False\n",
       "           ...  \n",
       "2999995    False\n",
       "2999996    False\n",
       "2999997    False\n",
       "2999998    False\n",
       "2999999    False\n",
       "Name: parent_id, Length: 1000000, dtype: bool"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_filter = chunk['parent_id'].isin(['fw0dv3f', 'fw0cet2'])\n",
    "reply_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4404460",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'author': 'str',\n",
    "         'created_utc': 'int64',\n",
    "         'permalink': 'str',\n",
    "         'score': 'int64',\n",
    "         'body': 'str',\n",
    "         'is_submitter': 'bool',\n",
    "         'id': 'str',\n",
    "         'link_id': 'str',\n",
    "         'parent_id': 'str',\n",
    "         'subreddit': 'str',\n",
    "         'subreddit_id': 'str'}\n",
    "\n",
    "# csv_path = r\"E:/Users/Christiaan/Large_Files/Thesis/reddit/comments/filtered/AAPL.csv\"\n",
    "# df = pd.read_csv(csv_path, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82d8cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 12)\n",
      "author              object\n",
      "created_utc          int64\n",
      "utc_datetime_str    object\n",
      "permalink           object\n",
      "score                int64\n",
      "body                object\n",
      "is_submitter          bool\n",
      "id                  object\n",
      "link_id             object\n",
      "parent_id           object\n",
      "subreddit           object\n",
      "subreddit_id        object\n",
      "dtype: object\n",
      "(1000000, 12)\n",
      "author              object\n",
      "created_utc          int64\n",
      "utc_datetime_str    object\n",
      "permalink           object\n",
      "score                int64\n",
      "body                object\n",
      "is_submitter          bool\n",
      "id                  object\n",
      "link_id             object\n",
      "parent_id           object\n",
      "subreddit           object\n",
      "subreddit_id        object\n",
      "dtype: object\n",
      "(1000000, 12)\n",
      "author              object\n",
      "created_utc          int64\n",
      "utc_datetime_str    object\n",
      "permalink           object\n",
      "score                int64\n",
      "body                object\n",
      "is_submitter          bool\n",
      "id                  object\n",
      "link_id             object\n",
      "parent_id           object\n",
      "subreddit           object\n",
      "subreddit_id        object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3620\\2603021669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1281\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1254\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1430\u001b[0m     \"\"\"\n\u001b[0;32m   1431\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\all_comments.csv\"\n",
    "\n",
    "chunksize = 10 ** 6\n",
    "for chunk in pd.read_csv(csv_path, chunksize=chunksize):\n",
    "    print(chunk.shape)\n",
    "    print(chunk.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df07c3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "if 'fzxo7s0' in df['id'].to_list():\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1847d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "source": [
    "if 'fzxo7s0' in df['id']:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a88fe2bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\\AAPL.csv\"\n",
    "df = pd.read_csv(csv_path, dtype=dtypes, parse_dates=['utc_datetime_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54010fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4597, 14)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\filtered\\AAPL.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb24222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>full_link</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>el_spidermonkey</td>\n",
       "      <td>1522725419</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/89...</td>\n",
       "      <td>898p8x</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>OLED has dropped 43% in the past three months ...</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "      <td>507760</td>\n",
       "      <td>OLED: Buy That Dip?</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/89...</td>\n",
       "      <td>2018-04-03 03:16:59</td>\n",
       "      <td>2018-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Timelapze</td>\n",
       "      <td>1522691994</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/89...</td>\n",
       "      <td>892e8d</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>When the S&amp;amp;P500 takes a dive at the hands ...</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "      <td>507476</td>\n",
       "      <td>Holding an S&amp;amp;P500 fund opens you up to an ...</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/89...</td>\n",
       "      <td>2018-04-02 17:59:54</td>\n",
       "      <td>2018-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trulytrulyisay</td>\n",
       "      <td>1522633918</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/88...</td>\n",
       "      <td>88vw7q</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>I’m curious to know of executives in the past ...</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "      <td>507103</td>\n",
       "      <td>Have there ever been company executives so suc...</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/88...</td>\n",
       "      <td>2018-04-02 01:51:58</td>\n",
       "      <td>2018-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ziadmiqdadi</td>\n",
       "      <td>1523335130</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/8b...</td>\n",
       "      <td>8b50z5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>They’re at an all year low and show promise fo...</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "      <td>510126</td>\n",
       "      <td>(STOCK)Thoughts on Cirrus Logic?</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/8b...</td>\n",
       "      <td>2018-04-10 04:38:50</td>\n",
       "      <td>2018-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zodiac2119</td>\n",
       "      <td>1524812210</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/8f...</td>\n",
       "      <td>8f9oxn</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>Hey all, First time posting here but I have a ...</td>\n",
       "      <td>investing</td>\n",
       "      <td>t5_2qhhq</td>\n",
       "      <td>515514</td>\n",
       "      <td>I have 15,000 sitting in a savings account. Bu...</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/8f...</td>\n",
       "      <td>2018-04-27 06:56:50</td>\n",
       "      <td>2018-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>Dolecavis04</td>\n",
       "      <td>1598545512</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>iho5mp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>FUCK this poopy headed, shitty stock that won’...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>1439962</td>\n",
       "      <td>Apple... FUCK</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>2020-08-27 16:25:12</td>\n",
       "      <td>2020-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>SanderGGs</td>\n",
       "      <td>1598540832</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>ihmp4a</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>Alright boys, I know it’s hard as fuck to move...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>1439584</td>\n",
       "      <td>Apple Price Forecast</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>2020-08-27 15:07:12</td>\n",
       "      <td>2020-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>spauldingzero</td>\n",
       "      <td>1598450187</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>igz9wt</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Never seen a split happen for calls I was hold...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>1434832</td>\n",
       "      <td>How will calls be affected for the Tesla and A...</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>2020-08-26 13:56:27</td>\n",
       "      <td>2020-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>M_lotta</td>\n",
       "      <td>1598437426</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>igwcux</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey guys, just wanted any advice relating to m...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>1434517</td>\n",
       "      <td>$TSLA &amp;amp; $AAPL Calls</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>2020-08-26 10:23:46</td>\n",
       "      <td>2020-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>bluechiphookahs</td>\n",
       "      <td>1598434218</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>igvrdq</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>End of the week run to the split- who else nee...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>1434465</td>\n",
       "      <td>🍎 to the 🌙 🚀 🚀 🚀</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>2020-08-26 09:30:18</td>\n",
       "      <td>2020-08-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4597 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  created_utc  \\\n",
       "0     el_spidermonkey   1522725419   \n",
       "1           Timelapze   1522691994   \n",
       "2      trulytrulyisay   1522633918   \n",
       "3         ziadmiqdadi   1523335130   \n",
       "4          Zodiac2119   1524812210   \n",
       "...               ...          ...   \n",
       "4592      Dolecavis04   1598545512   \n",
       "4593        SanderGGs   1598540832   \n",
       "4594    spauldingzero   1598450187   \n",
       "4595          M_lotta   1598437426   \n",
       "4596  bluechiphookahs   1598434218   \n",
       "\n",
       "                                              full_link      id  num_comments  \\\n",
       "0     https://www.reddit.com/r/investing/comments/89...  898p8x             7   \n",
       "1     https://www.reddit.com/r/investing/comments/89...  892e8d            24   \n",
       "2     https://www.reddit.com/r/investing/comments/88...  88vw7q            13   \n",
       "3     https://www.reddit.com/r/investing/comments/8b...  8b50z5             1   \n",
       "4     https://www.reddit.com/r/investing/comments/8f...  8f9oxn            15   \n",
       "...                                                 ...     ...           ...   \n",
       "4592  https://www.reddit.com/r/wallstreetbets/commen...  iho5mp             0   \n",
       "4593  https://www.reddit.com/r/wallstreetbets/commen...  ihmp4a            52   \n",
       "4594  https://www.reddit.com/r/wallstreetbets/commen...  igz9wt             8   \n",
       "4595  https://www.reddit.com/r/wallstreetbets/commen...  igwcux            60   \n",
       "4596  https://www.reddit.com/r/wallstreetbets/commen...  igvrdq            10   \n",
       "\n",
       "      score                                           selftext  \\\n",
       "0         3  OLED has dropped 43% in the past three months ...   \n",
       "1         7  When the S&amp;P500 takes a dive at the hands ...   \n",
       "2        18  I’m curious to know of executives in the past ...   \n",
       "3         0  They’re at an all year low and show promise fo...   \n",
       "4         3  Hey all, First time posting here but I have a ...   \n",
       "...     ...                                                ...   \n",
       "4592      1  FUCK this poopy headed, shitty stock that won’...   \n",
       "4593      1  Alright boys, I know it’s hard as fuck to move...   \n",
       "4594      1  Never seen a split happen for calls I was hold...   \n",
       "4595      1  Hey guys, just wanted any advice relating to m...   \n",
       "4596      1  End of the week run to the split- who else nee...   \n",
       "\n",
       "           subreddit subreddit_id  subreddit_subscribers  \\\n",
       "0          investing     t5_2qhhq                 507760   \n",
       "1          investing     t5_2qhhq                 507476   \n",
       "2          investing     t5_2qhhq                 507103   \n",
       "3          investing     t5_2qhhq                 510126   \n",
       "4          investing     t5_2qhhq                 515514   \n",
       "...              ...          ...                    ...   \n",
       "4592  wallstreetbets     t5_2th52                1439962   \n",
       "4593  wallstreetbets     t5_2th52                1439584   \n",
       "4594  wallstreetbets     t5_2th52                1434832   \n",
       "4595  wallstreetbets     t5_2th52                1434517   \n",
       "4596  wallstreetbets     t5_2th52                1434465   \n",
       "\n",
       "                                                  title  \\\n",
       "0                                   OLED: Buy That Dip?   \n",
       "1     Holding an S&amp;P500 fund opens you up to an ...   \n",
       "2     Have there ever been company executives so suc...   \n",
       "3                      (STOCK)Thoughts on Cirrus Logic?   \n",
       "4     I have 15,000 sitting in a savings account. Bu...   \n",
       "...                                                 ...   \n",
       "4592                                      Apple... FUCK   \n",
       "4593                               Apple Price Forecast   \n",
       "4594  How will calls be affected for the Tesla and A...   \n",
       "4595                            $TSLA &amp; $AAPL Calls   \n",
       "4596                                   🍎 to the 🌙 🚀 🚀 🚀   \n",
       "\n",
       "                                                    url             datetime  \\\n",
       "0     https://www.reddit.com/r/investing/comments/89...  2018-04-03 03:16:59   \n",
       "1     https://www.reddit.com/r/investing/comments/89...  2018-04-02 17:59:54   \n",
       "2     https://www.reddit.com/r/investing/comments/88...  2018-04-02 01:51:58   \n",
       "3     https://www.reddit.com/r/investing/comments/8b...  2018-04-10 04:38:50   \n",
       "4     https://www.reddit.com/r/investing/comments/8f...  2018-04-27 06:56:50   \n",
       "...                                                 ...                  ...   \n",
       "4592  https://www.reddit.com/r/wallstreetbets/commen...  2020-08-27 16:25:12   \n",
       "4593  https://www.reddit.com/r/wallstreetbets/commen...  2020-08-27 15:07:12   \n",
       "4594  https://www.reddit.com/r/wallstreetbets/commen...  2020-08-26 13:56:27   \n",
       "4595  https://www.reddit.com/r/wallstreetbets/commen...  2020-08-26 10:23:46   \n",
       "4596  https://www.reddit.com/r/wallstreetbets/commen...  2020-08-26 09:30:18   \n",
       "\n",
       "            date  \n",
       "0     2018-04-03  \n",
       "1     2018-04-02  \n",
       "2     2018-04-02  \n",
       "3     2018-04-10  \n",
       "4     2018-04-27  \n",
       "...          ...  \n",
       "4592  2020-08-27  \n",
       "4593  2020-08-27  \n",
       "4594  2020-08-26  \n",
       "4595  2020-08-26  \n",
       "4596  2020-08-26  \n",
       "\n",
       "[4597 rows x 14 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c691ddf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/wallstreetbets/comments/igwcux/tsla_aapl_calls/'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[4595,'full_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4980b568",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5121, 14)\n",
      "(4375, 14)\n",
      "(4233, 14)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates(subset=['author', 'date'], keep=False)\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop_duplicates(subset=['selftext'], keep=False)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe1e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "074e5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs [974         ] - Total counter [36244       ]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36244"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "rootdir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\posts\\filtered\"\n",
    "\n",
    "counter = 0\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        # Create csv_path\n",
    "        csv_path = os.path.join(subdir, file)\n",
    "        \n",
    "        \n",
    "        # Read csv\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        obs = df.shape[0]\n",
    "        counter = counter + obs\n",
    "        print(f\"Obs [{str(obs).ljust(12)}] - Total counter [{str(counter).ljust(12)}]\", end='\\r')\n",
    "\n",
    "            \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46f9cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38535, 16)\n",
      "(38535, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "38530    False\n",
       "38531    False\n",
       "38532    False\n",
       "38533    False\n",
       "38534    False\n",
       "Name: body, Length: 38534, dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_list = ['AAPL']\n",
    "save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\filtered\"\n",
    "\n",
    "csv_path = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\\investing\\2018_06.csv\"\n",
    "\n",
    "# Read csv\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(df.shape)\n",
    "df = df[df['body'].notna()]\n",
    "df['body'].apply(filter_data_1, ticker=ticker)\n",
    "# # Loop tickers\n",
    "# for ticker in ticker_list:\n",
    "#     print(f\"Processing {ticker.ljust(4)} for [{csv_path}]\", end='\\r')\n",
    "#     # Apply filter\n",
    "#     filtered_df = filter_mentions(df, ticker)\n",
    "\n",
    "#     # Creating save_path and saving file (either new file or appending)\n",
    "#     save_path = os.path.join(save_dir, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "#     save_df(filtered_df, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bef43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@loop_tickers\n",
    "def calculate_sentiment_ticker(*args, **kwargs):\n",
    "    print(args)\n",
    "    return \"yes\"\n",
    "#     # Setting up save location\n",
    "#     ticker = kwargs['ticker']\n",
    "#     save_dir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\Twitter\\sentiment\\VADER\"\n",
    "#     save_path = os.path.join(save_dir, f\"{ticker}.csv\").replace('\\\\', '/')\n",
    "    \n",
    "#     # Check if file already exists and skip sentiment calculation if file exists\n",
    "#     if os.path.isfile(save_path):\n",
    "#         print(f\"File exists: [{save_path}]\")\n",
    "        \n",
    "    \n",
    "#     else:  \n",
    "#         csv_path = kwargs['csv_path']\n",
    "\n",
    "#         # Read csv\n",
    "#         df = pd.read_csv(csv_path)\n",
    "\n",
    "#         # Filter and clean data. Also perform VADER sentiment scoring.\n",
    "#         return_df = clean_data(df, ticker)\n",
    "\n",
    "#         # Count the posts for each filter \n",
    "#         results_df = count_posts(return_df)\n",
    "\n",
    "#         # Calculate sentiment scores for each method\n",
    "#         sentiment_measures = calc_sent_measures(return_df)\n",
    "        \n",
    "#         # Saving the dataframe\n",
    "#         sentiment_measures.to_csv(save_path, encoding='utf-8', index=False)\n",
    "    \n",
    "\n",
    "filedir = r\"E:\\Users\\Christiaan\\Large_Files\\Thesis\\reddit\\comments\\unfiltered\"\n",
    "calculate_sentiment_ticker(file_dir= filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a84ecd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ATVI\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9548\\851111397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mticker_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing: {ticker.ljust(4)}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ticker_list = ['AAPL', 'AMD', 'AMZN', 'ATVI', 'BA', 'BABA', 'BAC', 'DIS', 'F', 'GE', 'GME', 'IQ', 'LULU', 'MSFT', 'MU', 'NFLX', 'NVDA', 'SBUX', 'SHOP', 'SNAP', 'SQ', 'TLRY', 'TSLA', 'V', 'WMT']\n",
    "import time\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    print(f\"Processing: {ticker.ljust(4)}\", end='\\r')\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1072017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
