{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b2169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pandas as pd\n",
    "import praw\n",
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI(num_workers=10)\n",
    "# api_praw = PushshiftAPI(praw=reddit)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85e127b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3999fce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PushshiftAPI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m api_praw \u001b[38;5;241m=\u001b[39m \u001b[43mPushshiftAPI\u001b[49m(praw\u001b[38;5;241m=\u001b[39mreddit)\n\u001b[0;32m      2\u001b[0m comments \u001b[38;5;241m=\u001b[39m api_praw\u001b[38;5;241m.\u001b[39msearch_comments(q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantum\u001b[39m\u001b[38;5;124m\"\u001b[39m, subreddit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscience\u001b[39m\u001b[38;5;124m\"\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, until\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1629990795\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(comments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m comments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PushshiftAPI' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "api_praw = PushshiftAPI(praw=reddit)\n",
    "comments = api_praw.search_comments(q=\"quantum\", subreddit=\"science\", limit=100, until=1629990795)\n",
    "print(f\"Found {len(comments)} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d16352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 0 comments from Reddit?\n"
     ]
    }
   ],
   "source": [
    "comments = api_praw.search_comments(q=\"quantum\", subreddit=\"science\", limit=100, before=1629990795)\n",
    "print(f'Retrieved {len(comments)} comments from Reddit?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41176314",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments:\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefaac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "if save:\n",
    "    # Directory where Reddit data is saved\n",
    "    reddit_dir = r\"C:/Users/Ck0rt/Documents/Large files/School/MSc Finance & Investments/Thesis/Reddit/posts\"\n",
    "\n",
    "    # Starting datetime\n",
    "    start_date = dt.datetime(2018, 4, 1, 0, 0)\n",
    "    end_date = dt.datetime(2020, 9, 1, 0, 0)\n",
    "\n",
    "    subreddits = ['wallstreetbets', 'stocks', 'investing', 'stockmarket', 'pennystocks']\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        # Create folder to save output\n",
    "        folder_loc = os.path.join(reddit_dir, subreddit).replace('\\\\', '/')\n",
    "        print(folder_loc)\n",
    "        Path(folder_loc).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        date_time = start_date\n",
    "        while date_time < end_date:\n",
    "            # Create 1 month search period in epoch time\n",
    "            year_and_month = date_time.strftime('%Y_%m')\n",
    "            start = int(date_time.timestamp())\n",
    "            end = date_time + relativedelta(months=1)\n",
    "            end = int(end.timestamp())\n",
    "\n",
    "            # Create file to save output\n",
    "            file_loc = os.path.join(folder_loc, year_and_month).replace('\\\\', '/') + \".csv\"\n",
    "\n",
    "            # Check if file already exists and skip API request if file exists\n",
    "            if os.path.isfile(file_loc):\n",
    "                print(f\"File exists: [{file_loc}]\")\n",
    "                date_time = date_time + relativedelta(months=1)\n",
    "\n",
    "                continue\n",
    "\n",
    "            # Api cooldown time\n",
    "            time.sleep(3)\n",
    "            print(f\"Now collecting data for [{subreddit}] in [{date_time.strftime('%B %Y')}]\")\n",
    "\n",
    "            # Request data from Pushshift\n",
    "            start_time = time.time()\n",
    "            posts = api.search_submissions(subreddit=subreddit, limit=300000, after=start, before=end)\n",
    "            print(f'Retrieved {len(posts)} posts from Pushshift in [{time.time() - start_time}] seconds')\n",
    "\n",
    "            # Save output to CSV via dataframe\n",
    "            reddit_df = pd.DataFrame(posts)\n",
    "\n",
    "            columns = ['author', 'created_utc', 'full_link', 'id', 'num_comments', 'score', 'selftext',\n",
    "                       'subreddit', 'subreddit_id', 'subreddit_subscribers', 'title', 'url']\n",
    "            reddit_df.to_csv(file_loc, header=True, index=False, columns=columns)\n",
    "            print(f\"Saving csv at [{file_loc}]\")\n",
    "\n",
    "            # Adding 1 month to date_time tracker\n",
    "            date_time = date_time + relativedelta(months=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
